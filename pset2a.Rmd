---
title: "ARE 213 Problem Set 2A"
author: "Becky Cardinali, Yuen Ho, Sara Johns, and Jacob Lefler"
date: "Due 10/26/2020"
header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath}
output: pdf_document
fig_caption: yes
geometry: margin=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#======================
# Section 0: Set Up
#======================

# Clear workspace
rm(list = ls())

# Load packages
library(pacman)
library(zoo)
library(haven)
p_load(data.table, dplyr, foreign, readstata13, tidyr, xtable, ggplot2, binom, glmnet, stats, FNN, fastDummies, fixest, parallel, plm, lmtest, fixest)

# set seed for replication
set.seed(1995)
# for mclapply
core.num <- (detectCores()/2)
# Directory 
base_directory <- "/Users/sarajohns/Desktop/ARE213_psets/"
#base_directory <- "/Users/beckycardinali/Desktop/ARE213_psets/"
# base_directory <- "/Users/yuen/Documents/GitHub/ARE213_psets/"
# base_directory <- "C:\\Users\\jacob\\Documents\\GitHub\\ARE213_psets\\"

# read data
traffic <- as.data.table(read_dta(paste0(base_directory, "traffic_safety2.dta")))

```

# Question 1

Question 10.3 from Wooldridge: For $T=2$ consider the standard unobserved effects model: 
\begin{equation}
y_{it} = \alpha + x_{it}\beta + c_i + u_{it}
\end{equation}
Let $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ represent the fixed effects and first differences estimators respectively. 

\bigskip

(a) Show that $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical. Hint: it may be easier to write $\hat{\beta}_{FE}$ as the "within estimator" rather than the fixed effects estimator. 

\bigskip

Define $\bar{Y}_i = \frac{1}{T}\sum_{t=1}^T Y_{it}$ and $\bar{X}_i = \frac{1}{T}\sum_{t=1}^T X_{it}$. Since $T=2$, we have $\bar{Y}_i = \frac{Y_{i1} + Y_{i2}}{2}$ and $\bar{X}_i = \frac{X_{i1} + X_{i2}}{2}$.  

Let $\ddot{Y}_{it} = Y_{it} - \bar{Y_i}$, $\ddot{X}_{it} = X_{it} - \bar{X_i}$, and $\ddot{\epsilon}_{it} = \epsilon_{it} - \bar{\epsilon_i}$.

The within estimator comes from running the following regression: $\ddot{Y}_{it} = \ddot{X}_{it}'\beta + \ddot{\epsilon}_{it}$. So the within estimator is given by $\hat{\beta}_{W} = (\ddot{X}_{it}'\ddot{X}_{it})^{-1}(\ddot{X}_{it}'\ddot{Y}_{it})$. As we know from Lecture Notes 3Ai (pg 12-13), the within estimator gives us estimates of $\beta$ that are numerically identical to those produced by the FE estimator. So we have $$\hat{\beta}_{FE} = \hat{\beta}_{W} = (\ddot{X}_{it}'\ddot{X}_{it})^{-1}(\ddot{X}_{it}'\ddot{Y}_{it})$$

Since $T=2$ we can rewrite this as $$\hat{\beta}_{FE} = \left[ \begin{pmatrix} \ddot{X}_{i1}' & \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{X}_{i1} \\ {\ddot{X}_{i2}} \end{pmatrix} \right]^{-1} \begin{pmatrix} \ddot{X}_{i1}' &  \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{Y}_{i1} \\ \ddot{Y}_{i2}\end{pmatrix} = \begin{pmatrix} \ddot{X}_{i1}'\ddot{X}_{i1} + \ddot{X}_{i2}'\ddot{X}_{i2}  \end{pmatrix}^{-1} \begin{pmatrix} \ddot{X}_{i1}'\ddot{Y}_{i1} + \ddot{X}_{i2}'\ddot{Y}_{i2}  \end{pmatrix}$$

Define $\Delta Y_{it} = Y_{it} - Y_{it-1}$, $\Delta X_{it} = X_{it} - X_{it-1}$, $\Delta \epsilon_{it} = \epsilon_{it} - \epsilon_{it-1}$. Then the regression $\Delta Y_{it} = \Delta X_{it}'\beta + \Delta \epsilon_{it}$ using data from time periods $2, ..., T$ yields the first differences estimator $\hat{\beta}_{FD} = (\Delta X_{it}'\Delta X_{it})^{-1}(\Delta X_{it}'\Delta Y_{it})$. Since we have $T=2$, $\Delta Y_{it} = Y_{i2} - Y_{i1}$, $\Delta X_{it} = X_{i2} - X_{i1}$, $\Delta \epsilon_{it} = \epsilon_{i2} - \epsilon_{i1}$

From the definition above, we have $\ddot{X}_{i1} = X_{i1} - \bar{X_i}$. Substituting in for $\bar{X}_i$ and then using the definition of $\Delta X_{it}$ gives us

$$\ddot{X}_{i1} = X_{i1} - \frac{X_{i1} + X_{i2}}{2} = \frac{X_{i1} - X_{i2}}{2} = \frac{-\Delta X_{it}}{2}$$
Similarly, 
$$\ddot{X}_{i2} = X_{i2} - \frac{X_{i1} + X_{i2}}{2} = \frac{X_{i2} - X_{i1}}{2} = \frac{\Delta X_{it}}{2}$$
$$\ddot{Y}_{i1} = Y_{i1} - \frac{Y_{i1} + Y_{i2}}{2} = \frac{Y_{i1} - Y_{i2}}{2} = \frac{-\Delta Y_{it}}{2}$$

$$\ddot{Y}_{i2} = Y_{i2} - \frac{Y_{i1} + Y_{i2}}{2} = \frac{Y_{i2} - Y_{i1}}{2} = \frac{\Delta Y_{it}}{2}$$
Substituting these values into $\hat{\beta}_{FE}$ gives
$$\hat{\beta}_{FE} = \left(\frac{-\Delta X'_{it}}{2}\frac{-\Delta X_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta X_{it}}{2}\right)^{-1} \left( \frac{-\Delta X'_{it}}{2}\frac{-\Delta Y_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta Y_{it}}{2}\right)$$ 
$$= \left(\frac{\Delta X'_{it}\Delta X_{it}}{4} + \frac{\Delta X_{it}'\Delta X_{it}}{4}\right)^{-1} \left( \frac{\Delta X_{it}'\Delta Y_{it}}{4} + \frac{\Delta X_{it}'\Delta Y_{it}}{4}\right)$$
$$= \left(\frac{\Delta X_{it}'\Delta X_{it}}{2}\right)^{-1} \left( \frac{\Delta X_{it}'\Delta Y_{it}}{2}\right) = (\frac{1}{2})^{-1}(\Delta X_{it}'\Delta X_{it})^{-1}(\frac{1}{2})(\Delta X_{it}'\Delta Y_{it})$$
$$= (\Delta X_{it}'\Delta X_{it})^{-1}(\Delta X_{it}'\Delta Y_{it}) = \hat{\beta}_{FD}$$
So for $T=2$, $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical.

\bigskip

(b) Show that the standard errors of $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical. If you wish, you may assume that $x_{it}$ is a scalar (i.e. there is only one regressor) and ignore any degree of freedom corrections. You are not clustering the standard errors in this problem. 

\bigskip

Ignoring any degree of freedom corrections, $Var(\hat{\beta}_{FE}) = \hat{\sigma}^2 (\ddot{X}'_{it}\ddot{X}_{it})^{-1}$ where $\hat{\sigma}^2$ is the sum of squared residuals.

For $\hat{\beta}_{FE}$ and $T=1$, 
$$\hat{\ddot{\epsilon}}_{i1} = \ddot{Y}_{i1} - \ddot{X}_{i1}\hat{\beta}_{FE} = \frac{-\Delta Y_{it}}{2} - \frac{-\Delta X_{it}}{2}\hat{\beta}_{FE}$$

And since we showed in part (a) that $\hat{\beta}_{FE} = \hat{\beta}_{FD}$
$$= \frac{-\Delta Y_{it}}{2} - \frac{-\Delta X_{it}}{2}\hat{\beta}_{FD} = (-\frac{1}{2})(\Delta Y_{it} - \Delta X_{it}\hat{\beta}_{FD}) = (-\frac{1}{2})\Delta \hat{\epsilon}_{it}$$
So for $T=1$, $(\hat{\ddot{\epsilon}}_{i1})^2 = \frac{1}{4}(\Delta \hat{\epsilon}_{it})^2$

Similarly, for $\hat{\beta}_{FE}$ and $T=2$, 
$$\hat{\ddot{\epsilon}}_{i2} = \ddot{Y}_{i2} - \ddot{X}_{i2}\hat{\beta}_{FE} = \frac{\Delta Y_{it}}{2} - \frac{\Delta X_{it}}{2}\hat{\beta}_{FE}$$

And since we showed in part (a) that $\hat{\beta}_{FE} = \hat{\beta}_{FD}$
$$= \frac{\Delta Y_{it}}{2} - \frac{\Delta X_{it}}{2}\hat{\beta}_{FD} = (\frac{1}{2})(\Delta Y_{it} - \Delta X_{it}\hat{\beta}_{FD}) = (\frac{1}{2})\Delta \hat{\epsilon}_{it}$$
So for $T=2$, $(\hat{\ddot{\epsilon}}_{i2})^2 = \frac{1}{4}(\Delta \hat{\epsilon}_{it})^2$

Therefore, the sum of squared residuals $\hat{\sigma}^2$ for $\hat{\beta}_{FE}$ is $(\hat{\ddot{\epsilon}}_{i1})^2 + (\hat{\ddot{\epsilon}}_{i2})^2 = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2$

Subsituting in for $\hat{\sigma}^2$ and $\ddot{X}_{it}$, we have $$Var(\hat{\beta}_{FE}) = \hat{\sigma}^2 (\ddot{X}'_{it}\ddot{X}_{it})^{-1} = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left[ \begin{pmatrix} \ddot{X}_{i1}' & \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{X}_{i1} \\ {\ddot{X}_{i2}} \end{pmatrix} \right]^{-1}$$
$$= \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2\begin{pmatrix} \ddot{X}_{i1}'\ddot{X}_{i1} + \ddot{X}_{i2}'\ddot{X}_{i2}  \end{pmatrix}^{-1}$$

$$= \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{-\Delta X'_{it}}{2}\frac{-\Delta X_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta X_{it}}{2}\right)^{-1}$$

$$=\frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{\Delta X'_{it}\Delta X_{it}}{4} + \frac{\Delta X_{it}'\Delta X_{it}}{4}\right)^{-1} = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{\Delta X_{it}'\Delta X_{it}}{2}\right)^{-1}$$
$$=\frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 (\frac{1}{2})^{-1}(\Delta X_{it}'\Delta X_{it})^{-1} = (\Delta \hat{\epsilon}_{it})^2 (\Delta X_{it}'\Delta X_{it})^{-1} = Var(\hat{\beta}_{FD})$$
since $(\Delta \hat{\epsilon}_{it})^2$ is the sum of squared residuals for $\hat{\beta}_{FD}$ because there are only 2 time periods, so $\Delta \epsilon_{it} = \epsilon_{i2} - \epsilon_{i1}$ is the only residual to square and include in the sum. We found that the variances are equal, and the standard error is just the square root of the variance.

So for $T=2$ and ignoring any degree of freedom corrections, the standard errors of $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical.

# Question 2

(a) We want to show that the fixed effects (FEs) estimator of $\beta$ in $y_{it} = \alpha + x_{it} \beta + \mu_i + \lambda_t + \epsilon_{it}$ can be obtained from two within transformations on this model. We show this by applying the Frisch-Waugh-Lovell (FWL) theorem. 

First, we let $R$ be the matrix of unit dummy variables, similar to the one defined in the lecture notes (the only difference being that we drop one dummy from $R$ to avoid perfect colinearity; for example, let $R_1 = \textbf{0}$). Further define $Q$ to be a matrix of time fixed effects, so $Q_{it,s} = \textbf{1}(t = s)$. (Note that for each observation $it$ we clearly have $R_{it} = R_i$ and $Q_{it} = Q_t$.) The fixed effects estimator would involve regressing $y_{it}$ on $X_{it}$, $R_i$ and $Q_t$. 

The FWL theorem, however, says that we can obtain the same coefficients from performing three simpler regressions. The first step is to regress $y_{it}$, $X_{it}$ and $R_{it}$ on $Q_{it}$ and obtain the residuals. This amounts to subtracting off the mean of each variable within each time period $t$. Define $\tilde{y}_{it} = y_{it} - \bar{y}_t$ where $\bar{y}_t := \frac{1}{T}\sum_t y_{it}$, and define $\tilde{X}_{it}$ and $\tilde{R}_{it}$ analogously.

Note that $\tilde{R}_{it} = $

# Question 3

(a) Run pooled bivariate OLS. Interpret. Add year fixed effects. Interpret. Add all covariates that you believe are appropriate. Think carefully about which covariates should be log transformed and which should enter in levels. What happens when you add these covariates? Why?

\bigskip

```{r}

# create y variable
traffic[, ln_fat_pc := log((fatalities/population))]
# log covariates
traffic[,ln_unemploy := log(unemploy)]
traffic[,ln_totalvmt := log(totalvmt)]
traffic[,ln_precip := log(precip)]
traffic[,ln_snow := log(snow32+0.01)] # to avoid NA from zeroes
# create dummies for FEs (to be used later)
traffic <- dummy_cols(traffic, select_columns = c("year", "state"))

# Pooled bivariate OLS
biv <- feols(ln_fat_pc ~ primary, data=traffic)
summary(biv, se="standard")

```

\bigskip

A naive bivariate OLS estimate suggests that having primary belt laws is associated with a `r round(biv$coefficients[[2]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus. Of course, we are likely omitting both observed and unobserved variables that affect selection into treatment (having primary belt laws), so this basic bivariate model simply gives us some descriptive information.  

\bigskip

```{r}
# Pooled bivariate OLS with year fixed effects
biv_yfe <- feols(ln_fat_pc ~ primary, fixef = "year", data=traffic)
summary(biv_yfe, se = "standard")
```

\bigskip

When we add year fixed effects to our pooled OLS model, we find that having primary belt laws is associated with a `r round(biv_yfe$coefficients[[1]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus. That is, some of the relationship between primary belt laws and traffic fatalities that we found in our bivariate model above can actually be explained by aggregate trends over time, which we control for when we include year fixed effects.

\bigskip

```{r}
# Pooled bivariate OLS with fixed effects and covariates
biv_yfe_cov <- feols(ln_fat_pc ~ primary + secondary + college + 
                       beer + ln_unemploy + ln_totalvmt + ln_precip +
                       ln_snow + rural_speed + urban_speed, fixef = "year", data=traffic)
summary(biv_yfe_cov, se = "standard")

```

\bigskip

Next we add covariates to our model, specifically secondary belt laws, percent college grads, per capita beer consumption, rural interstate speed limit, urban interstate speed limit, and logs of the unemployment rate, total vehicle miles travelled, percipitation, and snowfall. We decided to log those variables because they have a positively skewed distribution. 

\bigskip

With these additional covariates, our results indicate that having primary belt laws is associated with a `r round(biv_yfe_cov$coefficients[[1]]*(100), 2)`% increase in traffic fatalities per capita, ceteris paribus, which is not statistically significant. It seems unlikely that having primary belt laws would increase traffic fatalities, so it seems possible that our model suffers from omitted variable bias. For example, there may be state-specific, time-invariant characteristics which affect both the likelihood of having primary belt laws and traffic fatalities per capita. 

\bigskip

(b) Ignore omitted variables bias issues for the moment. Do you think the standard errors from above are right? Compute the Huber-White heteroskedasticity robust standard errors. Do they change much? Compute the clustered standard errors that are robust to within-state correlation. Do this using both the canned command and manually using the formulas we learned in class. Do the standard errors change much? Are you surprised? Interpret.

\bigskip

Our OLS standard errors above are only correct if we assume homoskedasticity and independence across observations. However, it seems likely that observations in different years within a given state will be positively correlated, even if we assume state-specific factors are independent across states. To account for heteroskedasticity we can calculate the Huber-White heteroskedasticity robust standard errors. To account for within-state correlation, we can calculate clustered standard errors at the state-level.

\bigskip

```{r}
# package command - heteroskedastic robust standard errors
summary(biv, se = "white")
summary(biv_yfe, se = "white")
summary(biv_yfe_cov, se = "white")

# package command - clustered standard errors
summary(biv, cluster  = traffic$state)
summary(biv_yfe, cluster = traffic$state)
summary(biv_yfe_cov, cluster = traffic$state)

```

\bigskip

Using the "canned" commands for calculating White-robust standard errors and clustered standard errors we find that our White-robust standard errors are generally slightly larger than our unadjusted OLS standard errors and that our clustered standard errors are larger than both the robust and unadjusted standard errors. We expect our clustered standard errors to be larger than our unclustered SEs the more we have intra-cluster correlation of regressors, the more observations we have per cluster, and the more correlated the errors are within-cluster. Intuitively, if errors are positively correlated within cluster then an additional observation in the cluster no longer provides a completely independent piece of new information. 

\bigskip

```{r}
# Manually calculate White-robust and clustered standard errors

# Manually obtain beta OLS matrix
calc.beta <- function(xmat, ymat) {
  (solve(t(xmat)%*%xmat)) %*% (t(xmat)%*%ymat)
}

white_middle <- function(xmat, ymat, beta) {
  residsq <- diag(as.vector((ymat - xmat %*% beta)^2))
  mid <- (t(xmat)%*%residsq%*%xmat)
  return(mid)
}

robust.se <- function(xmat, middle, adj) {
  
  var.robust <- adj * (solve(t(xmat)%*%xmat) %*% middle %*% solve(t(xmat)%*%xmat))
  
  se <- sqrt(diag(var.robust))
  
  return(se)
}

cluster_middle <- function(i, beta, DT, yvar, xvars) {
  
  state.xmat <- as.matrix(cbind(1,select(DT[state == i,], xvars)))
  state.ymat <- as.matrix(select(DT[state == i,], yvar))
  
  resid <- as.vector(state.ymat - state.xmat %*% beta)
  
  middle.term <- t(state.xmat) %*% resid %*% t(resid) %*% state.xmat
  
  return(middle.term)
}

# List of our variables for the three regressions
biv_var <- c("primary")
biv_yfe_var <- c("primary", colnames(traffic[,year_1982:year_2003]))
biv_yfe_cov_var <- c("primary", "secondary", "college", "beer",
                     "ln_unemploy", "ln_totalvmt", "ln_precip", 
                     "ln_snow", "rural_speed", "urban_speed", colnames(traffic[,year_1982:year_2003]))

# Run regression
xmat_biv <- as.matrix(cbind(1,select(traffic, all_of(biv_var))))
xmat_biv_yfe <- as.matrix(cbind(1, select(traffic, all_of(biv_yfe_var))))
xmat_biv_yfe_cov <- as.matrix(cbind(1, select(traffic, all_of(biv_yfe_cov_var))))
ymat <- as.matrix(select(traffic, ln_fat_pc))

beta_biv <- calc.beta(xmat_biv, ymat)
beta_biv_yfe <- calc.beta(xmat_biv_yfe, ymat)
beta_biv_yfe_cov <- calc.beta(xmat_biv_yfe_cov, ymat)

# Manually calculate White robust SEs
# get middle terms
w_mid_biv <- white_middle(xmat_biv, ymat, beta_biv)
w_mid_biv_yfe <- white_middle(xmat_biv_yfe, ymat, beta_biv_yfe)
w_mid_biv_yfe_cov <- white_middle(xmat_biv_yfe_cov, ymat, beta_biv_yfe_cov)
# adjustment factor (so that it matches HC1)
white_biv_adj <- nrow(xmat_biv)/(nrow(xmat_biv)- ncol(xmat_biv))
white_biv_yfe_adj <- nrow(xmat_biv_yfe)/(nrow(xmat_biv_yfe)- ncol(xmat_biv_yfe))
white_biv_yfe_cov_adj <- nrow(xmat_biv_yfe_cov)/(nrow(xmat_biv_yfe_cov)- ncol(xmat_biv_yfe_cov))
# get standard errors
white_biv <- robust.se(xmat_biv, w_mid_biv, white_biv_adj)
white_biv
white_biv_yfe <- robust.se(xmat_biv_yfe, w_mid_biv_yfe, white_biv_yfe_adj)
white_biv_yfe[1:2]
white_biv_yfe_cov <- robust.se(xmat_biv_yfe_cov, w_mid_biv_yfe_cov, white_biv_yfe_cov_adj)
white_biv_yfe_cov[1:11]

# Clustered by state
states <- as.vector(unique(traffic[,state])) # list of states
# adjustment factor (from section)
cl_biv_adj <- (length(states)/(length(states)-1))*((nrow(xmat_biv)-1)/(nrow(xmat_biv)- ncol(xmat_biv)))
cl_biv_yfe_adj <- (length(states)/(length(states)-1))*((nrow(xmat_biv_yfe)-1)/(nrow(xmat_biv_yfe)- ncol(xmat_biv_yfe)))
cl_biv_yfe_cov_adj <- (length(states)/(length(states)-1))*((nrow(xmat_biv_yfe_cov)-1)/(nrow(xmat_biv_yfe_cov)- ncol(xmat_biv_yfe_cov)))

cl_mid_biv_terms <- mclapply(states, cluster_middle, beta = beta_biv, DT = traffic, 
                             yvar="ln_fat_pc", xvars=biv_var, mc.cores = core.num)
cl_mid_biv <- Reduce('+', cl_mid_biv_terms)

cl_mid_biv_yfe_terms <- mclapply(states, cluster_middle, beta = beta_biv_yfe, DT = traffic, 
                                 yvar="ln_fat_pc", xvars=biv_yfe_var, mc.cores = core.num)
cl_mid_biv_yfe <- Reduce('+', cl_mid_biv_yfe_terms)

cl_mid_biv_yfe_cov_terms <- mclapply(states, cluster_middle, beta = beta_biv_yfe_cov, DT = traffic, 
                                     yvar="ln_fat_pc", xvars=biv_yfe_cov_var, mc.cores = core.num)
cl_mid_biv_yfe_cov <- Reduce('+', cl_mid_biv_yfe_cov_terms)

cl_biv <- robust.se(xmat_biv, cl_mid_biv, cl_biv_adj)
cl_biv
cl_biv_yfe <- robust.se(xmat_biv_yfe, cl_mid_biv_yfe, cl_biv_yfe_adj)
cl_biv_yfe[1:2]
cl_biv_yfe_cov <- robust.se(xmat_biv_yfe_cov, cl_mid_biv_yfe_cov, cl_biv_yfe_cov_adj)
cl_biv_yfe_cov[1:11]

```
\bigskip

When we calculate the White-robust and clustered standard errors by hand they are the same as the standard errors calculated by the "canned" commands above. We added adjustments to ensure of this. The White standard errors in the canned command are 'HC1' and use an adjustment factor of $\frac{N}{N-K}$. The clustered standard errors use an adjustment factor (also in the section notes) of $\frac{C}{C-1}\frac{N-1}{N-K}$. 

\bigskip

(c) Compute the between estimator, both with and without covariates. Under what conditions will this give an unbiased estimate of the effect of primary seat belt laws on fatalities per capita? Do you believe those conditions are met? Are you concerned about the standard errors in this case?

\bigskip

```{r}
# c - between estimator with and without covariates
traffic_bet <- traffic[, lapply(.SD, mean), by = "state"] # get means by state

between <- feols(ln_fat_pc ~ primary, data=traffic_bet)
summary(between, se="standard")
between_cov <- feols(ln_fat_pc ~ primary + secondary + college + 
                       beer + ln_unemploy + ln_totalvmt + ln_precip +
                       ln_snow + rural_speed + urban_speed, data=traffic_bet)
summary(between_cov, se = "standard")

```
\bigskip

Demeaning our data by state allows us to estimate the between estimator. Without covariates, our between estimator implies that having primary belt laws is associated with a `r round(between$coefficients[[2]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus. With covariates, our between estimator implies that having primary belt laws is associated with a `r round(between_cov$coefficients[[2]]*(100), 2)`% increase in traffic fatalities per capita, ceteris paribus. 

\bigskip

Under the strict exogeneity assumption, our between estimator gives us an unbiased estimate of the effect of primary seat belt laws on fatalities per capita, that is under the strict exogeneity assumption, $E[\ddot{\epsilon}_{it} | \ddot{X}_{it}] = 0$ where $\ddot{X}_{it} = X_{it} - \bar{X_i}$, and $\ddot{\epsilon}_{it} = \epsilon_{it} - \bar{\epsilon_i}$. The strict exogeneity assumption implies that the error term is uncorrelated with all past, present, and future values of the control variables, which is quite a strong assumption. In our model, this assumption is unlikely to hold if, for example, past snowfall affects current traffic fatalities per capita because in areas with lower past snowfall drivers are less experienced with driving safely during adverse weather conditions and are more likely to get into serious traffic accidents as a result. 

\bigskip

Note that our OLS standard errors from the between estimator are incorrect because $\ddot{\epsilon}_{it} = \epsilon_{it} - \bar{\epsilon_i}$ will be correlated across different observations within the same unit. However, we can obtain the correct standard errors by multiplying our OLS standard errors by $\sqrt{T / (T-1)}$.

\bigskip

(d) Compute the RE estimator (including covariates). Under what conditions will this give an unbiased estimate of the effect of primary seat belt laws on fatalities per capita? What are its advantages or disadvantages as compared to pooled OLS?

\bigskip

```{r}

# d - random effects estimator
random <- plm(ln_fat_pc ~ primary + secondary + college + 
                beer + ln_unemploy + ln_totalvmt + ln_precip +
                ln_snow + rural_speed + urban_speed, data=traffic, model="random")
summary(random)

```

\bigskip
Our random effects estimator implies that having primary belt laws is associated with a `r round(random$coefficients[[2]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus. Under the strict exogeneity assumption and the uncorrelated effects assumptions, that is the state-specific effect is uncorrelated with the regressors, the RE estimator is consistent. However if the uncorrelated effects assumption is violated, then our RE estimator will be biased. The benefit of the RE estimator is that if we can model the heteroskedasticity correctly it will be more efficient than pooled OLS. However, in order for the RE error structure to be correct, we need to assume that all residuals within a cluster are equally correlated with each other, which may not hold in practice. If this assumption does not hold, our RE estimator can still be consistent but may no longer be more efficient than OLS. 

\bigskip

(e) Do you think the standard errors from RE are right? Compute the clustered standard errors. Are they substantially different? If so, why? (i.e., what assumption(s) are being violated?)

\bigskip

As discussed above, the standard errors from our RE estimator above are only correct if we assume that all residuals within a cluster are equally correlated with each other, that is if we assume that the correlation between different observations for the same unit is always the same, regardless of how far apart in time they are. If this assumption is violated then our RE standard errors will be wrong. Indeed, when we compute the clustered SEs below they are generally larger than our RE standard errors, implying that our assumption that all residuals within a cluster are equally correlated with each other likely does not hold. 

```{r}

# e - clustered SEs
coeftest(random, vcovHC(random, type="HC1", cluster="group"))

```
\bigskip

(f) Compute the FE estimator using only primary and year fixed effects as the covariates. Compute the normal standard errors and the clustered standard errors. If they are different, why? 

\bigskip

```{r}
fixed <- plm(ln_fat_pc ~ primary, data = traffic, effect="twoways", index = c("state", "year"), model = "within")
summary(fixed)
coeftest(fixed, vcov = vcovHC(fixed, type = "HC1", cluster = "group"))
```

\bigskip

Our FE estimator implies that having primary belt laws is associated with `r round(fixed$coefficients[[1]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus. When we compute clustered standard errors they are almost double the size of our normal standard errors. As discussed in part b above, clustered SEs will be larger than unclustered SEs the more we have intra-cluster correlation of regressors, the more observations we have per cluster, and the more correlated the errors are within-cluster. 

\bigskip

(g) Add the same range of covariates to the FE estimator that you did to the OLS estimator. Are the FE estimates more or less stable than the OLS estimates? Why?

\bigskip

```{r}
fixed_cov <- plm(ln_fat_pc ~ primary + secondary + college + 
                beer + ln_unemploy + ln_totalvmt + ln_precip +
                ln_snow + rural_speed + urban_speed, data = traffic, effect="twoways", index = c("state", "year"), model = "within")
summary(fixed_cov)
coeftest(fixed_cov, vcov = vcovHC(fixed, type = "HC1", cluster = "group"))
```
\bigskip

When we add our full set of covariates to the FE estimator we find that having primary belt laws is associated with `r round(fixed_cov$coefficients[[1]]*(-100), 2)`% decrease in traffic fatalities per capita, ceteris paribus, which is statistically significant at the 1% level. These results are more stable than our OLS estimates in part a above because the sign is now in the expected direction and the effect is measured much more precisely. Thus it seems likely that our earlier OLS model suffered from omitted variable bias, namely from omitting state-specific effects, which we control for in our FE model. 

\bigskip

(h) Estimate a first-difference estimator, a 5-year differences estimator, and a long differences estimator, including year fixed effects (when feasible) and the appropriate covariates in each case. Briefly describe the pattern that emerges from the three differencing estimates. Where does the FE estimate fall in this pattern? Are you surprised?

\bigskip

The coefficient on primary seat belt law is growing in magnitude with first differences having the lowest and long differences having the greatest. First differences and five year differences are significant while long differences is not. This makes sense since long differences can only use one observation per state. The FE estimator is most similar to five year differences. This makes sense as the FE is an intermediary of the two extremes of first differences and long differences which is also one way we could think of the five year differences estimator.

```{r}
## First Differences Estimator
# Create first-differenced variables
setkey(traffic, state)
# lists of differenced vars
cols <- c("ln_fat_pc", "primary", "secondary", "college", "beer", "ln_unemploy", "ln_totalvmt", "ln_precip", "ln_snow", "rural_speed", "urban_speed")
fd_cols <- c("ln_fat_pc_fd", "primary_fd", "secondary_fd", "college_fd", "beer_fd", "ln_unemploy_fd", "ln_totalvmt_fd", "ln_precip_fd", "ln_snow_fd", "rural_speed_fd", "urban_speed_fd")
fived_cols <- c("ln_fat_pc_fived", "primary_fived", "secondary_fived", "college_fived", "beer_fived", "ln_unemploy_fived", "ln_totalvmt_fived", "ln_precip_fived", "ln_snow_fived", "rural_speed_fived", "urban_speed_fived")
ld_cols <- c("ln_fat_pc_ld", "primary_ld", "secondary_ld", "college_ld", "beer_ld", "ln_unemploy_ld", "ln_totalvmt_ld", "ln_precip_ld", "ln_snow_ld", "rural_speed_ld", "urban_speed_ld")

traffic[,c(fd_cols) := lapply(.SD, function(x){c(NA, diff(x, lag = 1))}), .SDcols = c(cols), by = state]
traffic[,c(fived_cols) := lapply(.SD, function(x){c(NA, NA, NA, NA, NA, diff(x, lag = 5))}), .SDcols = c(cols), by = state]
traffic[,c(ld_cols) := lapply(.SD, function(x){c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, diff(x, lag = 22))}), .SDcols = c(cols), by = state]

# Estimate first-differenced estimator with year FEs and covariates
fd <- feols(ln_fat_pc_fd ~ primary_fd + secondary_fd + college_fd + beer_fd + ln_unemploy_fd + ln_totalvmt_fd + ln_precip_fd + ln_snow_fd + rural_speed_fd + urban_speed_fd, fixef = "year", data = traffic[year > 1981])
summary(fd, cluster=traffic[year > 1981]$state)

# Estimate 5-year differences estimator with year FEs and covariates
fived <- feols(ln_fat_pc_fived ~ primary_fived + secondary_fived + college_fived + beer_fived + ln_unemploy_fived + ln_totalvmt_fived + ln_precip_fived + ln_snow_fived + rural_speed_fived + urban_speed_fived, fixef = "year", data = traffic[year > 1985])
summary(fived, cluster=traffic[year > 1985]$state)

# Estimate long differences estimator with covariates
ld <- feols(ln_fat_pc_ld ~ primary_ld + secondary_ld + college_ld + beer_ld + ln_unemploy_ld + ln_totalvmt_ld + ln_precip_ld + ln_snow_ld + rural_speed_ld + urban_speed_ld -1, data = traffic[year == 2003])
summary(ld, se = "white") 

```
\bigskip

(i) Make the case that the first-differences estimate is superior to the 5-year or long differences estimates.

\bigskip

The first differences estimate uses more data than the 5-year or long differences estimate. As such, it is able to take advantage of high frequency variation in our covariate over time for estimation. This is appealing from an identification perspective since we can control for unobserved trends which makes it less likely that they would drive our results. The 5-year or long differences estimates, in contrast, could be biased by unobserved trends that differ across units.

\bigskip

(j) Make the case that the 5-year or long differences estimates are superior to the first-differences estimate. 

\bigskip

The 5-year differences and long differences estimates avoid problems with measurement error in our covariates. In first differences, measurement error could be accentuated, causing our estimate to pick up noise rather than the true signal. With 5-year differences and long differences, we use a lower frequency and thus are more likely to pick up longer-term trends that we think are the true signal.
