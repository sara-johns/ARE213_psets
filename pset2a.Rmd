---
title: "ARE 213 Problem Set 2A"
author: "Becky Cardinali, Yuen Ho, Sara Johns, and Jacob Lefler"
date: "Due 10/26/2020"
header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath}
output: pdf_document
fig_caption: yes
geometry: margin=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#======================
# Section 0: Set Up
#======================

# Clear workspace
rm(list = ls())

# Load packages
library(pacman)
library(zoo)
library(haven)
p_load(data.table, dplyr, foreign, readstata13, tidyr, xtable, ggplot2, binom, glmnet, stats, FNN, fastDummies, fixest, parallel, plm, lmtest)

# set seed for replication
set.seed(1995)
core.num <- (detectCores()/2)
# Directory 
# base_directory <- "/Users/sarajohns/Desktop/ARE213_psets/"
base_directory <- "/Users/beckycardinali/Desktop/ARE213_psets/"
# base_directory <- "/Users/yuen/Documents/GitHub/ARE213_psets/"
# base_directory <- "C:\\Users\\jacob\\Documents\\GitHub\\ARE213_psets\\"

# read data
traffic <- as.data.table(read_dta(paste0(base_directory, "traffic_safety2.dta")))

```

# Question 1

Question 10.3 from Wooldridge: For $T=2$ consider the standard unobserved effects model: 
\begin{equation}
y_{it} = \alpha + x_{it}\beta + c_i + u_{it}
\end{equation}
Let $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ represent the fixed effects and first differences estimators respectively. 

(a) Show that $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical. Hint: it may be easier to write $\hat{\beta}_{FE}$ as the "within estimator" rather than the fixed effects estimator. 

Define $\bar{Y}_i = \frac{1}{T}\sum_{t=1}^T Y_{it}$ and $\bar{X}_i = \frac{1}{T}\sum_{t=1}^T X_{it}$. Since $T=2$, we have $\bar{Y}_i = \frac{Y_{i1} + Y_{i2}}{2}$ and $\bar{X}_i = \frac{X_{i1} + X_{i2}}{2}$.  

Let $\ddot{Y}_{it} = Y_{it} - \bar{Y_i}$, $\ddot{X}_{it} = X_{it} - \bar{X_i}$, and $\ddot{\epsilon}_{it} = \epsilon_{it} - \bar{\epsilon_i}$.

The within estimator comes from running the following regression: $\ddot{Y}_{it} = \ddot{X}_{it}'\beta + \ddot{\epsilon}_{it}$. So the within estimator is given by $\hat{\beta}_{W} = (\ddot{X}_{it}'\ddot{X}_{it})^{-1}(\ddot{X}_{it}'\ddot{Y}_{it})$. As we know from Lecture Notes 3Ai (pg 12-13), the within estimator gives us estimates of $\beta$ that are numerically identical to those produced by the FE estimator. So we have $$\hat{\beta}_{FE} = \hat{\beta}_{W} = (\ddot{X}_{it}'\ddot{X}_{it})^{-1}(\ddot{X}_{it}'\ddot{Y}_{it})$$

Since $T=2$ we can rewrite this as $$\hat{\beta}_{FE} = \left[ \begin{pmatrix} \ddot{X}_{i1}' & \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{X}_{i1} \\ {\ddot{X}_{i2}} \end{pmatrix} \right]^{-1} \begin{pmatrix} \ddot{X}_{i1}' &  \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{Y}_{i1} \\ \ddot{Y}_{i2}\end{pmatrix} = \begin{pmatrix} \ddot{X}_{i1}'\ddot{X}_{i1} + \ddot{X}_{i2}'\ddot{X}_{i2}  \end{pmatrix}^{-1} \begin{pmatrix} \ddot{X}_{i1}'\ddot{Y}_{i1} + \ddot{X}_{i2}'\ddot{Y}_{i2}  \end{pmatrix}$$

Define $\Delta Y_{it} = Y_{it} - Y_{it-1}$, $\Delta X_{it} = X_{it} - X_{it-1}$, $\Delta \epsilon_{it} = \epsilon_{it} - \epsilon_{it-1}$. Then the regression $\Delta Y_{it} = \Delta X_{it}'\beta + \Delta \epsilon_{it}$ using data from time periods $2, ..., T$ yields the first differences estimator $\hat{\beta}_{FD} = (\Delta X_{it}'\Delta X_{it})^{-1}(\Delta X_{it}'\Delta Y_{it})$. Since we have $T=2$, $\Delta Y_{it} = Y_{i2} - Y_{i1}$, $\Delta X_{it} = X_{i2} - X_{i1}$, $\Delta \epsilon_{it} = \epsilon_{i2} - \epsilon_{i1}$

From the definition above, we have $\ddot{X}_{i1} = X_{i1} - \bar{X_i}$. Substituting in for $\bar{X}_i$ and then using the definition of $\Delta X_{it}$ gives us

$$\ddot{X}_{i1} = X_{i1} - \frac{X_{i1} + X_{i2}}{2} = \frac{X_{i1} - X_{i2}}{2} = \frac{-\Delta X_{it}}{2}$$
Similarly, 
$$\ddot{X}_{i2} = X_{i2} - \frac{X_{i1} + X_{i2}}{2} = \frac{X_{i2} - X_{i1}}{2} = \frac{\Delta X_{it}}{2}$$
$$\ddot{Y}_{i1} = Y_{i1} - \frac{Y_{i1} + Y_{i2}}{2} = \frac{Y_{i1} - Y_{i2}}{2} = \frac{-\Delta Y_{it}}{2}$$

$$\ddot{Y}_{i2} = Y_{i2} - \frac{Y_{i1} + Y_{i2}}{2} = \frac{Y_{i2} - Y_{i1}}{2} = \frac{\Delta Y_{it}}{2}$$
Substituting these values into $\hat{\beta}_{FE}$ gives
$$\hat{\beta}_{FE} = \left(\frac{-\Delta X'_{it}}{2}\frac{-\Delta X_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta X_{it}}{2}\right)^{-1} \left( \frac{-\Delta X'_{it}}{2}\frac{-\Delta Y_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta Y_{it}}{2}\right)$$ 
$$= \left(\frac{\Delta X'_{it}\Delta X_{it}}{4} + \frac{\Delta X_{it}'\Delta X_{it}}{4}\right)^{-1} \left( \frac{\Delta X_{it}'\Delta Y_{it}}{4} + \frac{\Delta X_{it}'\Delta Y_{it}}{4}\right)$$
$$= \left(\frac{\Delta X_{it}'\Delta X_{it}}{2}\right)^{-1} \left( \frac{\Delta X_{it}'\Delta Y_{it}}{2}\right) = (\frac{1}{2})^{-1}(\Delta X_{it}'\Delta X_{it})^{-1}(\frac{1}{2})(\Delta X_{it}'\Delta Y_{it})$$
$$= (\Delta X_{it}'\Delta X_{it})^{-1}(\Delta X_{it}'\Delta Y_{it}) = \hat{\beta}_{FD}$$
So for $T=2$, $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical.

(b) Show that the standard errors of $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical. If you wish, you may assume that $x_{it}$ is a scalar (i.e. there is only one regressor) and ignore any degree of freedom corrections. You are not clustering the standard errors in this problem. 

Ignoring any degree of freedom corrections, $Var(\hat{\beta}_{FE}) = \hat{\sigma}^2 (\ddot{X}'_{it}\ddot{X}_{it})^{-1}$ where $\hat{\sigma}^2$ is the sum of squared residuals.

For $\hat{\beta}_{FE}$ and $T=1$, 
$$\hat{\ddot{\epsilon}}_{i1} = \ddot{Y}_{i1} - \ddot{X}_{i1}\hat{\beta}_{FE} = \frac{-\Delta Y_{it}}{2} - \frac{-\Delta X_{it}}{2}\hat{\beta}_{FE}$$

And since we showed in part (a) that $\hat{\beta}_{FE} = \hat{\beta}_{FD}$
$$= \frac{-\Delta Y_{it}}{2} - \frac{-\Delta X_{it}}{2}\hat{\beta}_{FD} = (-\frac{1}{2})(\Delta Y_{it} - \Delta X_{it}\hat{\beta}_{FD}) = (-\frac{1}{2})\Delta \hat{\epsilon}_{it}$$
So for $T=1$, $(\hat{\ddot{\epsilon}}_{i1})^2 = \frac{1}{4}(\Delta \hat{\epsilon}_{it})^2$

Similarly, for $\hat{\beta}_{FE}$ and $T=2$, 
$$\hat{\ddot{\epsilon}}_{i2} = \ddot{Y}_{i2} - \ddot{X}_{i2}\hat{\beta}_{FE} = \frac{\Delta Y_{it}}{2} - \frac{\Delta X_{it}}{2}\hat{\beta}_{FE}$$

And since we showed in part (a) that $\hat{\beta}_{FE} = \hat{\beta}_{FD}$
$$= \frac{\Delta Y_{it}}{2} - \frac{\Delta X_{it}}{2}\hat{\beta}_{FD} = (\frac{1}{2})(\Delta Y_{it} - \Delta X_{it}\hat{\beta}_{FD}) = (\frac{1}{2})\Delta \hat{\epsilon}_{it}$$
So for $T=2$, $(\hat{\ddot{\epsilon}}_{i2})^2 = \frac{1}{4}(\Delta \hat{\epsilon}_{it})^2$

Therefore, the sum of squared residuals $\hat{\sigma}^2$ for $\hat{\beta}_{FE}$ is $(\hat{\ddot{\epsilon}}_{i1})^2 + (\hat{\ddot{\epsilon}}_{i2})^2 = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2$

Subsituting in for $\hat{\sigma}^2$ and $\ddot{X}_{it}$, we have $$Var(\hat{\beta}_{FE}) = \hat{\sigma}^2 (\ddot{X}'_{it}\ddot{X}_{it})^{-1} = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left[ \begin{pmatrix} \ddot{X}_{i1}' & \ddot{X}_{i2}' \end{pmatrix} \begin{pmatrix} \ddot{X}_{i1} \\ {\ddot{X}_{i2}} \end{pmatrix} \right]^{-1}$$
$$= \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2\begin{pmatrix} \ddot{X}_{i1}'\ddot{X}_{i1} + \ddot{X}_{i2}'\ddot{X}_{i2}  \end{pmatrix}^{-1}$$

$$= \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{-\Delta X'_{it}}{2}\frac{-\Delta X_{it}}{2} + \frac{\Delta X'_{it}}{2}\frac{\Delta X_{it}}{2}\right)^{-1}$$

$$=\frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{\Delta X'_{it}\Delta X_{it}}{4} + \frac{\Delta X_{it}'\Delta X_{it}}{4}\right)^{-1} = \frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 \left(\frac{\Delta X_{it}'\Delta X_{it}}{2}\right)^{-1}$$
$$=\frac{1}{2}(\Delta \hat{\epsilon}_{it})^2 (\frac{1}{2})^{-1}(\Delta X_{it}'\Delta X_{it})^{-1} = (\Delta \hat{\epsilon}_{it})^2 (\Delta X_{it}'\Delta X_{it})^{-1} = Var(\hat{\beta}_{FD})$$
since $(\Delta \hat{\epsilon}_{it})^2$ is the sum of squared residuals for $\hat{\beta}_{FD}$ because there are only 2 time periods, so $\Delta \epsilon_{it} = \epsilon_{i2} - \epsilon_{i1}$ is the only residual to square and include in the sum. We found that the variances are equal, and the standard error is just the square root of the variance.

So for $T=2$ and ignoring any degree of freedom corrections, the standard errors of $\hat{\beta}_{FE}$ and $\hat{\beta}_{FD}$ are numerically identical.

# Question 2

# Question 3

## a - 

Run pooled bivariate OLS. Interpret. Add year fixed effects. Interpret. Add all covariates that you believe are appropriate. Think carefully about which covariates should be log transformed and which should enter in levels. What happens when you add these covariates? Why?

```{r}

# a- Pooled bivariate OLS , yr FE, All covariates 

# create y variable
traffic[, ln_fat_pc := log((fatalities/population))]
# log covariates
traffic[,ln_unemploy := log(unemploy)]
traffic[,ln_totalvmt := log(totalvmt)]
traffic[,ln_precip := log(precip)]
traffic[,ln_snow := log(snow32+0.01)] # to avoid NA from zeroes
# create dummies for FEs (to be used later)
traffic <- dummy_cols(traffic, select_columns = c("year", "state"))

# bivariate OLS
biv <- feols(ln_fat_pc ~ primary + secondary, data=traffic)
summary(biv, se="standard")

biv_yfe <- feols(ln_fat_pc ~ primary + secondary, fixef = "year", data=traffic)
summary(biv_yfe, se = "standard")

biv_yfe_cov <- feols(ln_fat_pc ~ primary + college + 
                       beer + ln_unemploy + ln_totalvmt + ln_precip +
                       ln_snow + rural_speed + urban_speed, fixef = "year", data=traffic)
summary(biv_yfe_cov, se = "standard")

```


## b - 

Ignore omitted variables bias issues for the moment. Do you think the standard errors from above are right? Compute the Huber-White heteroskedasticity robust standard errors. Do they change much? Compute the clustered standard errors that are robust to within-state correlation. Do this using both the canned command and manually using the formulas we learned in class. Do the standard errors change much? Are you surprised? Interpret.

```{r}
# b - white robust and clustered 

# package command - heteroskedastic
summary(biv, se = "white")
summary(biv_yfe, se = "white")
summary(biv_yfe_cov, se = "white")

# package command - cluster
summary(biv, cluster  = traffic$state)
summary(biv_yfe, cluster = traffic$state)
summary(biv_yfe_cov, cluster = traffic$state)

# write own commands

# will need to get beta matrix manually
calc.beta <- function(xmat, ymat) {
  (solve(t(xmat)%*%xmat)) %*% (t(xmat)%*%ymat)
}

white_middle <- function(xmat, ymat, beta) {
  residsq <- diag(as.vector((ymat - xmat %*% beta)^2))
  mid <- (t(xmat)%*%residsq%*%xmat)
  return(mid)
}

robust.se <- function(xmat, middle) {
  
  var.robust <- solve(t(xmat)%*%xmat) %*% middle %*% solve(t(xmat)%*%xmat)
  
  se <- sqrt(diag(var.robust))
  
  return(se)
}

cluster_middle <- function(i, beta, DT, yvar, xvars) {
  
  state.xmat <- as.matrix(cbind(1,select(DT[state == i,], xvars)))
  state.ymat <- as.matrix(select(DT[state == i,], yvar))
  
  resid <- as.vector(state.ymat - state.xmat %*% beta)
  
  middle.term <- t(state.xmat) %*% resid %*% t(resid) %*% state.xmat
  
  return(middle.term)
}

# List of our variables for the three regressions
biv_var <- c("primary", "secondary")
biv_yfe_var <- c("primary", "secondary", colnames(traffic[,year_1982:year_2003]))
biv_yfe_cov_var <- c("primary", "secondary", "college", "beer",
                     "ln_unemploy", "ln_totalvmt", "ln_precip", 
                     "ln_snow", "rural_speed", "urban_speed", colnames(traffic[,year_1982:year_2003]))

# Run regression
xmat_biv <- as.matrix(cbind(1,select(traffic, all_of(biv_var))))
xmat_biv_yfe <- as.matrix(cbind(1, select(traffic, all_of(biv_yfe_var))))
xmat_biv_yfe_cov <- as.matrix(cbind(1, select(traffic, all_of(biv_yfe_cov_var))))
ymat <- as.matrix(select(traffic, ln_fat_pc))

beta_biv <- calc.beta(xmat_biv, ymat)
beta_biv_yfe <- calc.beta(xmat_biv_yfe, ymat)
beta_biv_yfe_cov <- calc.beta(xmat_biv_yfe_cov, ymat)

# White robust
# get middle terms
w_mid_biv <- white_middle(xmat_biv, ymat, beta_biv)
w_mid_biv_yfe <- white_middle(xmat_biv_yfe, ymat, beta_biv_yfe)
w_mid_biv_yfe_cov <- white_middle(xmat_biv_yfe_cov, ymat, beta_biv_yfe_cov)
# get standard errors
white_biv <- robust.se(xmat_biv, w_mid_biv)
white_biv
white_biv_yfe <- robust.se(xmat_biv_yfe, w_mid_biv_yfe)
white_biv_yfe[1:3]
white_biv_yfe_cov <- robust.se(xmat_biv_yfe_cov, w_mid_biv_yfe_cov)
white_biv_yfe_cov[1:11]

# Clustered by state
states <- as.vector(unique(traffic[,state]))

cl_mid_biv_terms <- mclapply(states, cluster_middle, beta = beta_biv, DT = traffic, 
                             yvar="ln_fat_pc", xvars=biv_var, mc.cores = core.num)
cl_mid_biv <- Reduce('+', cl_mid_biv_terms)

cl_mid_biv_yfe_terms <- mclapply(states, cluster_middle, beta = beta_biv_yfe, DT = traffic, 
                                 yvar="ln_fat_pc", xvars=biv_yfe_var, mc.cores = core.num)
cl_mid_biv_yfe <- Reduce('+', cl_mid_biv_yfe_terms)

cl_mid_biv_yfe_cov_terms <- mclapply(states, cluster_middle, beta = beta_biv_yfe_cov, DT = traffic, 
                                     yvar="ln_fat_pc", xvars=biv_yfe_cov_var, mc.cores = core.num)
cl_mid_biv_yfe_cov <- Reduce('+', cl_mid_biv_yfe_cov_terms)

cl_biv <- robust.se(xmat_biv, cl_mid_biv)
cl_biv
cl_biv_yfe <- robust.se(xmat_biv_yfe, cl_mid_biv_yfe)
cl_biv_yfe[1:3]
cl_biv_yfe_cov <- robust.se(xmat_biv_yfe_cov, cl_mid_biv_yfe_cov)
cl_biv_yfe_cov[1:11]

```

## c -

Compute the between estimator, both with and without covariates. Under what conditions will this give an unbiased estimate of the effect of primary seat belt laws on fatalities per capita? Do you believe those conditions are met? Are you concerned about the standard errors in this case?

```{r}
# c - between estimator with and without covariates
traffic_bet <- traffic[, lapply(.SD, mean), by = "state"] # get means by state

between <- feols(ln_fat_pc ~ primary + secondary, data=traffic_bet)
summary(between, se="standard")
between_cov <- feols(ln_fat_pc ~ primary + secondary + college + 
                       beer + ln_unemploy + ln_totalvmt + ln_precip +
                       ln_snow + rural_speed + urban_speed, data=traffic_bet)
summary(between_cov, se = "standard")

```

## d - 

Compute the RE estimator (including covariates). Under what conditions will this give an unbiased estimate of the effect of primary seat belt laws on fatalities per capita? What are its advantages or disadvantages as compared to pooled OLS?

```{r}

# d - random effects estimator
random <- plm(ln_fat_pc ~ primary + secondary + college + 
                beer + ln_unemploy + ln_totalvmt + ln_precip +
                ln_snow + rural_speed + urban_speed, data=traffic, model="random")
summary(random)

```

## e - 

Do you think the standard errors from RE are right? Compute the clustered standard errors. Are they substantially different? If so, why? (i.e., what assumption(s) are being violated?)

```{r}

# e - clustered SEs
coeftest(random, vcovHC(random, type="sss", cluster="group"))

```