---
title: "ARE 213 Problem Set 2B"
author: "Becky Cardinali, Yuen Ho, Sara Johns, and Jacob Lefler"
date: "Due 11/16/2020"
header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath}
output: pdf_document
fig_caption: yes
geometry: margin=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#======================
# Section 0: Set Up
#======================

# Clear workspace
rm(list = ls())

# Load packages
library(pacman)
library(zoo)
library(haven)
p_load(coefplot, data.table, dplyr, foreign, readstata13, tidyr, xtable, broom, stringr, glue,
       ggplot2, stats, FNN, fastDummies, fixest, parallel, plm, lmtest, fixest, Synth, sandwich, SCtools)


# Directory 
#base_directory <- "/Users/sarajohns/Desktop/ARE213_psets/"
# base_directory <- "/Users/beckycardinali/Desktop/ARE213_psets/"
base_directory <- "/Users/yuen/Documents/GitHub/ARE213_psets/"
# base_directory <- "C:\\Users\\jacob\\Documents\\GitHub\\ARE213_psets\\"

# read data
traffic <- as.data.table(read_dta(paste0(base_directory, "traffic_safety2.dta")))
```

# Question 1

We first estimate an event study specification.

(a) First determine the minimum and maximum event time values that you can estimate in this data set. Code up a separate event time indicator for each possible value of event time in the data set. Estimate an event study regression using all the event time indicators. What happens?

The data set contains data for each year in $[1981, 2003]$. 

The latest year that any state adopted a primary seat belt law was 2003, so the minimum event time value we can estimate in this dataset is -22. We know this because year 2003 is coded as event time 0 in this case, and there are 22 periods in $[1981, 2002]$ which come before the year of adoption so they are coded with negative event time values.

The earliest year that any state adopted a primary seat belt law was 1984, so the maximum event time value we can estimate in this dataset is 19. We know this because year 1984 is coded as event time 0 in this case, and there are 19 periods in $[1985, 2003]$ which come after the year of adoption so they are coded with positive event time values.

In the case of a single treated state $(s = 1)$ and a single control state $(s = 0)$, for an event study regression using all the event time indicators, we would estimate the regression: 
$$Y_{ist} = \alpha + \sum_{j=-T_0}^{T-T_0} \tau_j D_{jst} + \gamma_s + \delta_t + \epsilon_{st} + u_{ist}$$
where $T_0$ is the period just prior to treatment, and $D_{jst}$ is an indicator function for period $t$ falling $j$ periods after $T_0$ in the treated state (i.e., $\textbf{1}(t-T_0 = j)*\textbf{1}(s=1)$), so index $j$ is "event time." In our case, we have multiple treated states with event dates that vary across units, so our regression specification will be slightly different, particularly our event time indicators will be interacted with an "ever treated" indicator for each state.

```{r, include = TRUE}

# separate event time indicator for each possible value of event time in the data set

# create event time variable
traffic <- traffic[, event_time := NA]

# iterate through all states (this includes state 99)
for(s in unique(traffic$state)){

  # data frame for just state s
  temp <- subset(traffic, state == s)
  # make sure it's in ascending order by year
  temp <- temp[order(year), ]

  # find the row # and year corresponding to the first occurrence of primary == 1 
  # in the dataframe for just that state
  row_of_first_occ <- match(1, temp$primary)
  first_treated_year <- traffic[match(1, temp$primary), year]
  
  # case where a state was never treated in the sample (row_of_first_occ == NA)
  # make all event times very negative (-1000) for these never treated states
  
  if(is.na(row_of_first_occ)){
    
    # all event_time values get coded as -1000
    setDT(traffic)[, event_time := ifelse(state == s, -1000, event_time)]
  }
  
  # case where a state was treated in the sample (row_of_first_occ > 1)
  # there are no cases of a state being treated during the entire sample
  else{
    # for the first year with primary == 1, make event time = 0
    setDT(traffic)[, event_time := ifelse(state == s & year == first_treated_year, 0, event_time)]

    k <- 1
    for(i in (row_of_first_occ+1):23){
      # year after event time 0 corresponds to event time 1, 
      # 2 years after event time 0 corresponds to event time 2, etc.
      setDT(traffic)[, event_time := ifelse(state == s & year == (first_treated_year + k), k, event_time)]
      k <- k + 1
      }

    k <- -1
    for(i in (row_of_first_occ-1):1){
      # year before event time 0 corresponds to event time -1, 
      # 2 years before event time 0 corresponds to event time -2, etc.
      setDT(traffic)[, event_time := ifelse(state == s & year == (first_treated_year + k), k, event_time)]
      k <- k - 1
      }
  }
}

# remove 99 for this part of the analysis
traf_pt1 <- traffic[!(state==99),]

# make separate event time indicator for each possible value of event time in the data set 
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("event_time"))]

# get ever treated dummy
traf_pt1[,treat:=max(primary), by = state]
# interact with ever treated
event_time_cols <- names(traf_pt1[,`event_time_-1`:`event_time_19`]) # get list of event time columns
# multiply ever treated by all event time columns
traf_pt1 <- traf_pt1[, (event_time_cols) := lapply(.SD, function(x) x * traf_pt1$treat), 
                     .SDcols = event_time_cols]

# create y variable
traf_pt1[, ln_fat_pc := log((fatalities/population))]

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg <- traf_pt1[,c("ln_fat_pc", "state", "year",..event_time_cols)]
traf_reg[,state:=factor(state)]
traf_reg[,year:=factor(year)]

# event study regression using all the event time indicators
es1 <- lm(ln_fat_pc ~ .-`event_time_-1000`, data = traf_reg)
es1_cluster <- vcovCL(es1, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es1, vcov. = es1_cluster)

# lm chooses to remove event time 19

```

When we estimate an event study regression using all the event time indicators, the $\texttt{lm}$ function in R chooses to remove event time 19. It makes sense that R omitted one event time indicator because in practice, one cannot estimate the regression using all the event time indicators because of the dummy variable trap. An event study regression using all the event time indicators contains $T$ dummy variables, which fully saturates event time and leads to the dummy variable trap.

(b) Estimate another event study regression using all the event time indicators save one that you choose to omit. Generate a plot of the event study coefficients. 

```{r, include = TRUE}

# omit event_time -1
es2 <- lm(ln_fat_pc ~ .-`event_time_-1`-`event_time_-1000`, data = traf_reg)
es2_cluster <- vcovCL(es2, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es2, vcov. = es2_cluster)

es2_table <- data.table(tidy(coeftest(es2, vcov. = es2_cluster)))

es2_table <- es2_table[grep("event_time", term),]
es2_table[, event_time := as.numeric(str_match(es2_table$term, "([0-9]{1,2})")[, 2])]
es2_table[grep("-", term), event_time := -1 * event_time]
es2_table[, ci_upper := estimate + 1.96*std.error]
es2_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es2_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()

```

(c) Create minimum and maximum event time indicators that correspond to bins of event time $< -5$ and event time $> 5$ respectively. Appropriately specify and estimate an event study regression using these min and max event time indicators. Generate a plot of the event study coefficients. Explain which specification you prefer, this one or the one in part (b). 

```{r, include = TRUE}

# create min and max event time indicators for event time < -5 and > 5
traf_pt1 <- traf_pt1[, bin_event_time := event_time]

# make any event time > 5 equal to 6 (max event time indicator)
setDT(traf_pt1)[, bin_event_time := ifelse(event_time > 5, 6, bin_event_time)]

# make any event time < -5 equal to -6 (min event time indicator)
# this means never treated states (which have event time -1000) will be in the
# min event time group, but this doesn't change the coefficient on the min 
# event time dummy 
setDT(traf_pt1)[, bin_event_time := ifelse(event_time < -5, -6, bin_event_time)]


# make separate bin event time indicator for each possible value of bin event time in the data set 
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("bin_event_time"))]

# interact with ever treated
bin_event_time_cols <- names(traf_pt1[,`bin_event_time_-1`:`bin_event_time_6`]) # get list of bin event time columns
# multiply ever treated by all bin event time columns
traf_pt1 <- traf_pt1[, (bin_event_time_cols) := lapply(.SD, function(x) x * traf_pt1$treat), 
                     .SDcols = bin_event_time_cols]

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg3 <- traf_pt1[,c("ln_fat_pc", "state", "year",..bin_event_time_cols)]
traf_reg3[,state:=factor(state)]
traf_reg3[,year:=factor(year)]

# event study regression using all the bin event time indicators except -1
es3 <- lm(ln_fat_pc ~ .-`bin_event_time_-1`, data = traf_reg3)
es3_cluster <- vcovCL(es3, type = "HC1", cluster=traf_reg3$county) # get cluster cov-var
coeftest(es3, vcov. = es3_cluster)

es3_table <- data.table(tidy(coeftest(es3, vcov. = es3_cluster)))

es3_table <- es3_table[grep("event_time", term),]
es3_table[, event_time := as.numeric(str_match(es3_table$term, "([0-9]{1,2})")[, 2])]
es3_table[grep("-", term), event_time := -1 * event_time]
es3_table[, ci_upper := estimate + 1.96*std.error]
es3_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es3_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()

```

There are tradeoffs between this specification (with min and max bins) and the specification in part (b) (without min and max bins). The specification in part (b) allows us to look at estimates for more years before and after treatment. However, the very large and very small values of event time may include information from only a few states since the distribution of event time is unbalanced across units in our sample. Early-treated units do not have very early event time dummies, and late-treated units do not have very late event time dummies. 

The min and max bins in this specification (c) partially address the unbalanced distribution of event time across units, but this specification only shows us 6 periods before and 6 periods after treatment. Additionally, the cutoffs for min and max in this specification can seem arbitrary. Nevertheless, as we will see in part (d), the min and max bins in this specification help avoid collinearity that can occur with the specification in part (b) if there are no "pure control" states. So overall, this specification with min and max bins seems preferable.

(d) What happens to your estimates from part (b) if you exclude the "pure control" states from your sample? What about if you exclude the pure controls in part (c)?

```{r, include = TRUE}

# exclude "pure control" states from the sample
traf_pt1_no_pure <- traf_pt1[!(event_time==-1000),]

# redo part (b) with no "pure control" states

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg <- traf_pt1_no_pure[,c("ln_fat_pc", "state", "year",..event_time_cols)]
traf_reg[,state:=factor(state)]
traf_reg[,year:=factor(year)]

es_b <- lm(ln_fat_pc ~ .-`event_time_-1`, data = traf_reg)
es_b_cluster <- vcovCL(es_b, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es_b, vcov. = es_b_cluster)

# lm chooses to remove event time 19
# (even though we already removed event time -1)
```

If we exclude the "pure control" states from our sample, the $\texttt{lm}$ function in R chooses to remove event time 19, even though we already removed one event time dummy (event time -1). It makes sense that R omitted an additional event time dummy because without any "pure control" states, there is collinearity since event time is equal to calendar time minus the treatment date. When there are no "pure control" states, the full set of event time indicators (with one omitted) plus time and treatment date fixed effects (treatment date fixed effects are proxied by state fixed effects) will be collinear and thus result in the dummy variable trap, which is why it makes sense that R removed an additional event time dummy.


```{r, include = TRUE}
# redo part (c) with no "pure control" states

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg3 <- traf_pt1_no_pure[,c("ln_fat_pc", "state", "year",..bin_event_time_cols)]
traf_reg3[,state:=factor(state)]
traf_reg3[,year:=factor(year)]

# event study regression using all the bin event time indicators except -1
es_c <- lm(ln_fat_pc ~ .-`bin_event_time_-1`, data = traf_reg3)
es_c_cluster <- vcovCL(es_c, type = "HC1", cluster=traf_reg3$county) # get cluster cov-var
coeftest(es_c, vcov. = es_c_cluster)

es_c_table <- data.table(tidy(coeftest(es_c, vcov. = es_c_cluster)))

es_c_table <- es_c_table[grep("event_time", term),]
es_c_table[, event_time := as.numeric(str_match(es_c_table$term, "([0-9]{1,2})")[, 2])]
es_c_table[grep("-", term), event_time := -1 * event_time]
es_c_table[, ci_upper := estimate + 1.96*std.error]
es_c_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es_c_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()
```

If we exclude the "pure control" states from our sample, the estimates as compared to part (c) are slightly different, especially for the larger and smaller event time bins, and more coefficients were statistically significant in (c) than here. But overall, the estimates are not that different from (c). Importantly, there is no collinearity in this min and max bin specification, even when there are no "pure control" states, which shows that collinearity was avoided using the specification with min and max bins, but it was not avoided when redoing the specification in (b) with no "pure control" states. 

(e) Overall, does the event study regression make you more confident or less confident that seat belt laws reduce fatalities (relative to the fixed effects results that you estimated on the last problem set)? Briefly explain. 

Last problem set, we found that when we add our full set of covariates to the FE estimator, having primary belt laws is associated with 8.98% decrease in traffic fatalities per capita, ceteris paribus, which is statistically significant at the 1% level. Overall, the event study regression makes us less confident that seat belt laws reduce fatalities (relative to our fixed effects results that we estimated last problem set). From the lecture notes, for event studies, an abrupt change in the outcome immediately after treatment is often viewed as more credible than a gradual change over time, because it is less likely to be due to differential trends between the treated and control states. Our plots of the event study coefficients do not show an abrupt (or statistically significant) change in the outcome immediately after treatment, or even in the first few periods after treatment. So our event study regression results make us less confident. However, our event study regression specification may need to be modified further before interpreting the results, and we will do one such modification in part (f).

(f) Building off the event study regression from part (c), estimate the interaction weighted event study estimator from Sun and Abraham (2020) (discussed in lecture and posted on bCourses for refrence). As a reminder, the interacted event study regression takes the standard event time indicators (without any binning) and interacts each one with a cohort indicator (a cohort refers to a group of states that share the same date on which they were first treated). You then form the estimate for event time coefficient $\tau_j$ by averaging the estimates of the cohort-specific $\tau_j$ using the weights described in Sun and Abraham (2020).

We estimate the interaction weighted event study estimator below. We first interact a cohort dummy with each event time dummy that is in the range for that cohort (e.g for treatment cohort 2000, the dummy would be interacted with [-19,3]). We then put all of the interactions in a regression, omitting the never treated cohort and the -1 event time. The output of this regression gives us a coefficient for each event time by cohort. We then take a weighted average of the coefficients by event time so that we get one coefficient per event time. The weights are the share that each cohort contributes to the event time (e.g. the number of states in each cohort represented by that event time). We then take the weighted average for times $t < -5$ and $t > 5$ (just represented by -6 and 6 respectively in the graph). Similarly the weights are the share that each event time contributes to the pooled event time (e.g. the number of states for each $t \in [-22,-6]$ and $t \in [6, 19]$). We plot the results and see that they are similar to the results in (c).

```{r}
## f interaction weighted
# get cohorts
traf_pt1[event_time==0,cohort:=year]
traf_pt1[,cohort:= lapply(.SD, function(x) replace(x, is.na(x), mean(x, na.rm=T))), .SDcols = "cohort", by = state]
traf_pt1[is.na(cohort), cohort:=2103]
# get dummy for each cohort
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("cohort"))]
cohort_cols <- names(traf_pt1[,cohort_1984:cohort_2003]) # get list of bin event time columns

traf_reg_iw <- traf_pt1[,c("ln_fat_pc", "state", "event_time", "cohort", "year",..event_time_cols,..cohort_cols)]
traf_reg_iw[,state:=factor(state)]
traf_reg_iw[,year:=factor(year)]

# get interactions
reg_iw_interactions <- function(x) {
  
  dt <- traf_reg_iw[cohort==x,]
  min_et <- 1981 - x # min interaction
  max_et <- 2003 - x # max interaction
  
  # don't create interactions for never treated
  if (x != 2103) {
    # loop through all in between min and max
    for (i in min_et:max_et) {
      dt[, glue("int_{x}_{i}") := eval(as.name(glue("event_time_{i}"))) * eval(as.name(glue("cohort_{x}")))]
    }
  }
  
  else {
  }
  
  return(dt)
}

cohorts <- unique(traf_reg_iw$cohort) # unique cohorts
int_dt_list <- lapply(cohorts, reg_iw_interactions) # lapply through all cohorts
int_dt <- rbindlist(int_dt_list, fill = T) # get as table
int_cols <- names(int_dt[,grep("int", names(int_dt)), with = F])
int_dt[,(int_cols):= lapply(.SD, function(x) replace(x, is.na(x), 0)), .SDcols = int_cols]
traf_reg_iw_final <-  int_dt[,c("ln_fat_pc", "state", "year",..int_cols)]

# run regression. omitted cohort is never treated. omitted event time is -1
es_iw <- lm(ln_fat_pc ~. - `int_2000_-1` - `int_1993_-1` - `int_1986_-1` - `int_2003_-1` -
              `int_1996_-1` - `int_1998_-1` - `int_1987_-1` - `int_1984_-1` - `int_1991_-1` - `int_2002_-1`, 
            data = traf_reg_iw_final)

# get results
iw_tab <- data.table(tidy(coeftest(es_iw)))
iw_tab <- iw_tab[grep("int", term),]
iw_tab[, cohort := as.numeric(str_extract(iw_tab$term, "([0-9]{4})"))]
iw_tab[, event_time := -1*as.numeric(str_match(iw_tab$term, "_-([0-9]{1,2})")[,2])]
iw_tab[is.na(event_time), event_time := as.numeric(str_match(iw_tab[is.na(event_time),term], "([0-9]{1,2})$")[,2])]

# get number of states
state_dt <- traf_pt1[,.(cohort = mean(cohort)), by = state]
state_dt <- state_dt[,.(num_states = .N), by = cohort]

iw_tab_merge <- merge(iw_tab, state_dt, by = "cohort")
# weighted average across cohorts
iw_tab_weight1 <- iw_tab_merge[,.(estimate = weighted.mean(estimate, w = num_states),
                                  num_states = sum(num_states)), by = event_time]
# weighted average for t < -5 and t > 5
iw_tab_weight1[,time_bin := event_time]
iw_tab_weight1[event_time < -5,time_bin := -6]
iw_tab_weight1[event_time > 5,time_bin := 6]

iw_tab_weight2 <- iw_tab_weight1[,.(estimate = weighted.mean(estimate, w = num_states)), by = time_bin]

# plot
ggplot(data=iw_tab_weight2, aes(x=time_bin, y=estimate)) +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Interaction Weighted Event Study") +
  theme_bw()

```


# Question 2

We now apply the synthetic control methods from Abadie et al (2010).

(a) Please use the aggregate "treatment" state (a population weighted average of the first 4 states to have a primary seatbelt law: CT, IA, NM, TX) as the treatment unit (TU) in the synthetic control analysis. 

i. Compare the average pre-period log traffic fatalities per capita of the TU site to that of the average of all the “control” states. Next, graph the pre-period log traffic fatalities by year for the pre-period for both the TU and the average of the control group. Interpret.

\bigskip

```{r}
traffic <- as.data.table(read_dta(paste0(base_directory, "traffic_safety2.dta")))

#Create a treatment status variable that = 1 if state is CT, IA, NM, or TX and =0 otherwise. 
traffic[,treat := ifelse(state_name == "CT" | state_name == "IA"|state_name == "NM" | state_name == "TX" |state == 99, 1, 0)]

controls <- traffic[primary == 0 & year == 2003,state] #select states that never pass a primary seat belt law to serve as potential controls

traffic_SYM <- traffic[traffic$state %in% controls | state == 99,] #Create new data table with only the potential controls and the TU

#Set state_name for state 99 to TU
traffic_SYM[state == 99, state_name := "TU"]

#Change treatment variable to a factor variable
traffic_SYM$treat <- as.factor(traffic_SYM$treat)

#Create log fatalities per capita variable
traffic_SYM[, ln_fat_pc := log(fatalities/population)]

# Create log covariates
traffic_SYM[,ln_unemploy := log(unemploy)]
traffic_SYM[,ln_totalvmt := log(totalvmt)]
traffic_SYM[,ln_precip := log(precip)]
traffic_SYM[,ln_snow := log(snow32+0.01)] # to avoid NA from zeroes

#Compare the average pre-period log traffic fatalities per capita between treatment and control
premeanT <- mean(traffic_SYM[treat == 1 & year<1986, ln_fat_pc]) #mean pre-period log traffic fatalities in treatment
premeanC <- mean(traffic_SYM[treat == 0 & year<1986, ln_fat_pc]) #mean pre-period log traffic fatalities in control

#Create variable of mean log traffic fatalities by treatment status by year
traffic_SYM[, mean_lnfat_treat := lapply(.SD, mean), .SDcols = c("ln_fat_pc"), by = c("treat","year")] 

#Graph the mean pre-period log traffic fatalities by year for Treatment vs Control
traffic_SYM[year < 1986,] %>%
  ggplot(aes(x=year, y = mean_lnfat_treat, group = treat, color = treat)) + 
  geom_line() +
  theme_minimal() +
  labs(title = "Average Pre-Period Log Traffic Fatalities by Year", x = "Year", y = "Log Traffic Fatalities Per Capita", color = element_blank()) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(labels = c("Controls", "TU"), values = c("coral1", "cyan3"))

```

The average pre-period log traffic fatalities per capita in our aggregate treatment unit is `r round(premeanT,2)` compared with `r round(premeanC,2)` in our control states. Graphically, we can see that while log traffic fatalities per capita are declining over time in both groups, treatment units have on average higher traffic fatalities per capita than control units in all pre-period years. This makes sense since states with higher traffic fatalities are more likely to take steps to reduce such fatalities, leading to a higher likelihood of implementing seat belt laws.  

\bigskip

ii. Compare the dependent variable between the TU and each control state for the year before the treatment. Which control state best matches the TU? Now compare this state's covariates with the TU covariates. Do they appear similar? What might this imply in terms of using this state as the counterfactual state? 

\bigskip

```{r}
#generate a variable that is the absolute value of the difference between the dependent variable of the TU site and each control state for the year before treatment 
TU_dep_1985 <- traffic_SYM[state == 99 & year == 1985, ln_fat_pc] #1985 log traffic fatalities per capita for TU
traffic_SYM[year == 1985 & state != 99,compare_dep := abs(ln_fat_pc - TU_dep_1985)]
traffic_SYM[is.na(compare_dep), compare_dep := 999]
control <- traffic_SYM[compare_dep == min(traffic_SYM$compare_dep),state] #WV best matches the TU

#Compare average of WV's covariates to TU's covariates in pre-period
avg_comparisons <- data.table(covariates = colnames(traffic_SYM[,c(3:4, 7:13, 17)]), WV_avg = colMeans(traffic_SYM[state == 47 & year < 1986,c(3:4, 7:13, 17)]), TU_avg = colMeans(traffic_SYM[state == 99 & year < 1986, c(3:4, 7:13, 17)]))
avg_comparisons[,c("WV_avg", "TU_avg")] <- round(avg_comparisons[,c("WV_avg", "TU_avg")], 2) #round numeric values to 2 decimal places

avg_comparisons

```

\bigskip
When we compare log traffic fatalities per capita in 1985 (the year before treatment), we find that West Virginia is the state closest to the TU. We then compare the average pre-period values for various covariates between West Virginia and TU in the table above and find that the two are not very comparable. For example, West Virginia has on average a lower percentage of college grads, per capita beer consumption, population, and total vehicle miles traveled (VMT) than the TU and on average higher unemployment, precipitation, and snowfall. This suggests that West Virginia is not a good counterfactual state since it differs from the TU systematically in the pre-period. 

\bigskip

(b) Apply the synthetic control method using the available covariates and pre-treatment outcomes to construct a synthetic control group.

\bigskip
i. Discuss the synthetic control method including its benefits and potential drawbacks. 

\bigskip
Compared to a diff-in-diff estimator, the synthetic control estimator has the benefit of providing a more rigorous, less ad-hoc way of selecting control units from a large pool of potential controls. Unlike the diff-in-diff estimator, the synthetic control estimator also does not reply on the assumption of parallel preimplementation trends.  A key advantage is that a synthetic control estimator controls for both observed and unobserved unit-by-time shocks, whereas a diff-in-diff estimator only controls for observed unit-by-time shocks. To see this intuitively, note that only units that are similar in both observed and unobserved determinants of the outcome variable and in the effect of those determinants on the outcome variable will produce similar trajectories of the outcome variable over extended periods of time (Abadie et al. 2010). With a synthetic controls estimator, we can also leverage the large pool of potential controls to conduct permutation-based inference.  

\bigskip

One drawback, however, is that the credibility of the synthetic controls method relies on achieving a good preimplementation fit for the outcome of interest between treated unit and synthetic control, which is difficult if the treated unit is an outlier. This also necessitates having enough data on the outcome and covariate variables for the treated unit and a suitable pool of comparison units over a significant period of time. Furthermore, judging whether there is a good fit is more of an art than a science, as there is currently no consensus on what constitutes a "good fit." A related drawback is that the synthetic control units need to match the treated unit on both levels and trends. Thus, if there are control units that are only a good match for the trends but not the levels, or vice versa, then we may be discarding control units that satisfy the parallel trends assumption because they do not match the baseline levels. We also need to assume that there are no shocks that affect the treated unit differentially than the potential control units and that there are no spillovers of the treatment effect from the treated unit into the control units, although these are also necessary assumptions for a diff-in-diff estimator. 

\bigskip
ii. Use the software package provided by Abadie et al. to apply the synthetic control method. Please be sure to state precisely what the command is doing and how you determined your preferred specification. 

\bigskip

We use the R package "Synth" to implement the synthetic control method. Synth constructs a synthetic control group by searching for a weighted combination of control units chosen to approximate the treated unit (TU) in terms of characteristics that are predictive of the outcome. Synth requires us to supply four matrices as its main arguments, X0, X1, Z1, and Z0. X1 and X0 contain the predictor values for the TU and the control units respectively. Similarly, Z1 and Z0 contain the outcome variable for the pre-intervention period for the TU and the control units respectively. The command "dataprep" allows us to create the matrices X1, X0, Z1, and Z0. The command "synth" is then used to construct the synthetic control group, where weights are assigned to the control units to minimize the mean squared prediction error (MSPE) over the pre-intervention time period (in our case 1981-1985). We then use the command "gaps.plot" to plot the gaps in the trajectories of the outcome variable for the TU and the constructed synthetic control group. 

```{r}

## Specification A
dataprep.outA <- dataprep(foo = traffic_SYM,
                         predictors = c("college","unemploy","totalvmt","precip"),
                         predictors.op = "mean",
                         dependent = "ln_fat_pc",
                         unit.variable = "state",
                         time.variable = "year",
                         special.predictors = list(
                            list("ln_fat_pc", 1981, "mean"),
                            list("ln_fat_pc", 1983, "mean"),
                            list("ln_fat_pc", 1985, "mean")),
                         treatment.identifier = 99,
                         controls.identifier = controls,
                         time.predictors.prior = c(1981:1985),
                         time.optimize.ssr = c(1981:1985),
                         unit.names.variable = "state_name",
                         time.plot = 1981:2003
                         )
# synth command identifies weights to construct the synthetic control 
synth.outA <- synth(dataprep.outA)

# plot the gaps (treated - synthetic)
gaps.plot(dataprep.res = dataprep.outA,
          synth.res = synth.outA,
          Ylab = "Gap: Treated - Synthetic",
          Xlab = "Year",
          Main = "Specification A")

## Specification B - Include more covariates and log of covariates
dataprep.outB <- dataprep(foo = traffic_SYM,
                         predictors = c("college","ln_unemploy","ln_totalvmt","ln_precip", "beer", "population", "ln_snow"),
                         predictors.op = "mean",
                         dependent = "ln_fat_pc",
                         unit.variable = "state",
                         time.variable = "year",
                         special.predictors = list(
                            list("ln_fat_pc", 1981, "mean"),
                            list("ln_fat_pc", 1983, "mean"),
                            list("ln_fat_pc", 1985, "mean")),
                         treatment.identifier = 99,
                         controls.identifier = controls,
                         time.predictors.prior = c(1981:1985),
                         time.optimize.ssr = c(1981:1985),
                         unit.names.variable = "state_name",
                         time.plot = 1981:2003
                         )

# synth command identifies weights to construct the synthetic control 
synth.outB <- synth(dataprep.outB)

# plot the gaps (treated - synthetic)
gaps.plot(dataprep.res = dataprep.outB,
          synth.res = synth.outB,
          Ylab = "Gap: Treated - Synthetic",
          Xlab = "Year",
          Main = "Specification B")

## Specification C - include more covariates and more "special predictors" (Preferred Specification)
dataprep.outC <- dataprep(foo = traffic_SYM,
                         predictors = c("college","ln_unemploy","ln_totalvmt","ln_precip", "beer", "population", "ln_snow"),
                         predictors.op = "mean",
                         dependent = "ln_fat_pc",
                         unit.variable = "state",
                         time.variable = "year",
                         special.predictors = list(
                            list("ln_fat_pc", 1981, "mean"),
                            list("ln_fat_pc", 1983, "mean"),
                            list("ln_fat_pc", 1984, "mean"),
                            list("ln_fat_pc", 1985, "mean")),
                         treatment.identifier = 99,
                         controls.identifier = controls,
                         time.predictors.prior = c(1981:1985),
                         time.optimize.ssr = c(1981:1985),
                         unit.names.variable = "state_name",
                         time.plot = 1981:2003
                         )
# run the synth command to identify weights
synth.outC<- synth(dataprep.outC)

# plot the gaps (treated - synthetic)
gaps.plot(dataprep.res = dataprep.outC,
          synth.res = synth.outC,
          Ylab = "Gap: Treated - Synthetic",
          Xlab = "Year",
          Main = "Specification C - Preferred Specification")

```

\bigskip

We run the synthetic control method for a variety of specifications, varying which variables we include as control variables and varying the number of "special predictors" (i.e. the mean of the outcome variable in chosen pre-intervention years) we include. Our preferred specification is specification "C", which includes the most covariates and "special predictors," since the gap between the TU and the synthetic control in the pre-intervention period is modestly closer to 0 with this specification, particularly in the years closest to the treatment year. 

\bigskip
(c) Graphical interpretation and treatment significance

\bigskip
i. Generate graphs plotting the gap between the TU and the synthetic control group under both your preferred specification and a few other specifications you tried.

\bigskip

```{r,include=FALSE, cache = TRUE}
## Specification A

## run the generate.placebos command to reassign treatment status
## to each unit listed as control, one at a time, and generate their
## synthetic versions. Sigf.ipop = 2 for faster computing time. 
## Increase to the default of 5 for better estimates. 
tdfA <- generate.placebos(dataprep.outA,
                  synth.outA,
                  Sigf.ipop = 2,
                  strategy = "sequential"
                  )

## Specification B
tdfB <- generate.placebos(dataprep.outB,
                  synth.outB,
                  Sigf.ipop = 2,
                  strategy = "sequential"
                  )

## Specification C - Preferred Specification
tdfC <- generate.placebos(dataprep.outC,
                  synth.outC,
                  Sigf.ipop = 2,
                  strategy = "sequential"
                  )
```
```{r, include = TRUE}
## Plot the gaps in outcome values over time of each unit --
# treated and placebos -- to their synthetic controls

#Specification A
pA <- plot_placebos(tdfA,
                  discard.extreme=FALSE, 
                  mspe.limit=20,
                  title = "Specification A",
                  xlab='Year',
                  ylab='Gap in per capita log fatalities',
                  alpha.placebos = 1)
pA

#Specification B
pB <- plot_placebos(tdfB,
                  discard.extreme=FALSE, 
                  mspe.limit=20,
                  title = "Specification B",
                  xlab='Year',
                  ylab='Gap in per capita log fatalities',
                  alpha.placebos = 1)
pB

#Specification C - Preferred Specification
pC <- plot_placebos(tdfC,
                  discard.extreme=FALSE, 
                  mspe.limit=20, 
                  title = "Specification C - Preferred Specification",
                  xlab='Year',
                  ylab='Gap in per capita log fatalities',
                  alpha.placebos = 1)
pC
```

ii. Compare the graph plotting the gap between the TU and the synthetic control group under your preferred specification with the graphs plotting the gap between each control state and its "placebo" treatment. Do you conclude that the treatment was significant? Why or why not?

\bigskip

Comparing the above graphs, we conclude that the treatment was not significant. Since we constructed the synthetic control unit so that it tracked the TU closely in the pre-intervention period, we expect the two units to diverge in the post-intervention period more than in the pre-intervention period by construction, even if there is no treatment effect. When we compare the observed treatment effect to the "placebo" treatment effects for the control states, the treatment effect for the TU is near the middle of the distribution, suggesting that our measured treatment effect could be simply due to chance. If we had randomly picked an untreated state and implemented the same procedure, it is likely we would have found a post-intervention deviation of the observed magnitude or larger. 

\bigskip

iii. Create a graph of the post-treatment/pre-treatment prediction ratios of the Mean Squared Prediction Errors (MSPE) for the actual and "placebo" treatment gaps in (ii). Do you conclude that the treatment was significant? Why or why not?

\bigskip

```{r, warning = FALSE}
## Specification A
mspe.plot(tdfA,
          discard.extreme = TRUE,
          mspe.limit = 20,
          plot.hist = TRUE,
          title = "Specification A",
          xlab = "Post/Pre MSPE Ratio",
          ylab = "Frequency"
          )

## Specification B
mspe.plot(tdfB,
          discard.extreme = TRUE,
          mspe.limit = 20,
          plot.hist = TRUE,
          title = "Specification B",
          xlab = "Post/Pre MSPE Ratio",
          ylab = "Frequency"
          )

## Specification C
mspe.plot(tdfC,
          discard.extreme = TRUE,
          mspe.limit = 20,
          plot.hist = TRUE,
          title = "Specification C - Preferred Specification",
          xlab = "Post/Pre MSPE Ratio",
          ylab = "Frequency"
          )
```

\bigskip

Based on the above graphs of the Post/Pre MSPE Ratio for our three specifications, we conclude that the treatment was not significant. In our preferred specification (C), 13 control states obtain the same Post/Pre MSPE ratio as the TU or larger. Thus we calculate a p-value of $p = 0.43$ ($13/30$), that is if we were to assign the intervention at random in the data, the probability of obtaining and Post/Pre MSPE ratio as large as the TU's is 0.43. Based on these results, we cannot reject the null hypothesis of no treatment effect. 

\bigskip
(d) How do your synthetic control results compare to your fixed effects results from Question (3) in the last problem set? Interpret any differences. 

\bigskip

From the last problem set, Q3 Part f, the results from our FE estimator with all covariates indicated that primary seat belt laws are associated with a $8.98\%$ decrease in log fatalities per capita, ceteris paribus, which is statistically significant at the 1\% level. This contrasts with our findings using the synthetic control method, where we cannot reject the null hypothesis that primary seat belt laws have no effect on log fatalities per capita. The difference in results could arise from a few factors. One, the results from our synthetic control model may be less credible if we think our constructed synthetic control does not have a good enough pre-implementation fit. As we can see from part b above, across all of our specifications the gap between the TU and the synthetic control is small (less than 0.1) but not very close to 0, particularly for time periods further away from the implementation year. Two, it could be that our FE estimator suffers from omitted variable bias, whereas our synthetic controls estimator also controls for unobserved state-by-year shocks. If this is the case, then our synthetic controls estimator would be less biased than our FE estimator from the previous problem set. 