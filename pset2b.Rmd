---
title: "ARE 213 Problem Set 2B"
author: "Becky Cardinali, Yuen Ho, Sara Johns, and Jacob Lefler"
date: "Due 11/16/2020"
header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath}
output: pdf_document
fig_caption: yes
geometry: margin=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#======================
# Section 0: Set Up
#======================

# Clear workspace
rm(list = ls())

# Load packages
library(pacman)
library(zoo)
library(haven)
p_load(coefplot, data.table, dplyr, foreign, readstata13, tidyr, xtable, broom, stringr, glue,
       ggplot2, stats, FNN, fastDummies, fixest, parallel, plm, lmtest, fixest, Synth, sandwich)


# Directory 
# base_directory <- "/Users/sarajohns/Desktop/ARE213_psets/"
base_directory <- "/Users/beckycardinali/Desktop/ARE213_psets/"
# base_directory <- "/Users/yuen/Documents/GitHub/ARE213_psets/"
# base_directory <- "C:\\Users\\jacob\\Documents\\GitHub\\ARE213_psets\\"

# read data
traffic <- as.data.table(read_dta(paste0(base_directory, "traffic_safety2.dta")))
```

# Question 1

We first estimate an event study specification.

(a) First determine the minimum and maximum event time values that you can estimate in this data set. Code up a separate event time indicator for each possible value of event time in the data set. Estimate an event study regression using all the event time indicators. What happens?

The data set contains data for each year in $[1981, 2003]$. 

The latest year that any state adopted a primary seat belt law was 2003, so the minimum event time value we can estimate in this dataset is -22. We know this because year 2003 is coded as event time 0 in this case, and there are 22 periods in $[1981, 2002]$ which come before the year of adoption so they are coded with negative event time values.

The earliest year that any state adopted a primary seat belt law was 1984, so the maximum event time value we can estimate in this dataset is 19. We know this because year 1984 is coded as event time 0 in this case, and there are 19 periods in $[1985, 2003]$ which come after the year of adoption so they are coded with positive event time values.

In the case of a single treated state $(s = 1)$ and a single control state $(s = 0)$, for an event study regression using all the event time indicators, we would estimate the regression: 
$$Y_{ist} = \alpha + \sum_{j=-T_0}^{T-T_0} \tau_j D_{jst} + \gamma_s + \delta_t + \epsilon_{st} + u_{ist}$$
where $T_0$ is the period just prior to treatment, and $D_{jst}$ is an indicator function for period $t$ falling $j$ periods after $T_0$ in the treated state (i.e., $\textbf{1}(t-T_0 = j)*\textbf{1}(s=1)$), so index $j$ is "event time." In our case, we have multiple treated states with event dates that vary across units, so our regression specification will be slightly different, particularly our event time indicators will be interacted with an "ever treated" indicator for each state.

```{r, include = TRUE}

# separate event time indicator for each possible value of event time in the data set

# create event time variable
traffic <- traffic[, event_time := NA]

# iterate through all states (this includes state 99)
for(s in unique(traffic$state)){

  # data frame for just state s
  temp <- subset(traffic, state == s)
  # make sure it's in ascending order by year
  temp <- temp[order(year), ]

  # find the row # and year corresponding to the first occurrence of primary == 1 
  # in the dataframe for just that state
  row_of_first_occ <- match(1, temp$primary)
  first_treated_year <- traffic[match(1, temp$primary), year]
  
  # case where a state was never treated in the sample (row_of_first_occ == NA)
  # make all event times very negative (-1000) for these never treated states
  
  if(is.na(row_of_first_occ)){
    
    # all event_time values get coded as -1000
    setDT(traffic)[, event_time := ifelse(state == s, -1000, event_time)]
  }
  
  # case where a state was treated in the sample (row_of_first_occ > 1)
  # there are no cases of a state being treated during the entire sample
  else{
    # for the first year with primary == 1, make event time = 0
    setDT(traffic)[, event_time := ifelse(state == s & year == first_treated_year, 0, event_time)]

    k <- 1
    for(i in (row_of_first_occ+1):23){
      # year after event time 0 corresponds to event time 1, 
      # 2 years after event time 0 corresponds to event time 2, etc.
      setDT(traffic)[, event_time := ifelse(state == s & year == (first_treated_year + k), k, event_time)]
      k <- k + 1
      }

    k <- -1
    for(i in (row_of_first_occ-1):1){
      # year before event time 0 corresponds to event time -1, 
      # 2 years before event time 0 corresponds to event time -2, etc.
      setDT(traffic)[, event_time := ifelse(state == s & year == (first_treated_year + k), k, event_time)]
      k <- k - 1
      }
  }
}

# remove 99 for this part of the analysis
traf_pt1 <- traffic[!(state==99),]

# make separate event time indicator for each possible value of event time in the data set 
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("event_time"))]

# get ever treated dummy
traf_pt1[,treat:=max(primary), by = state]
# interact with ever treated
event_time_cols <- names(traf_pt1[,`event_time_-1`:`event_time_19`]) # get list of event time columns
# multiply ever treated by all event time columns
traf_pt1 <- traf_pt1[, (event_time_cols) := lapply(.SD, function(x) x * traf_pt1$treat), 
                     .SDcols = event_time_cols]

# create y variable
traf_pt1[, ln_fat_pc := log((fatalities/population))]

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg <- traf_pt1[,c("ln_fat_pc", "state", "year",..event_time_cols)]
traf_reg[,state:=factor(state)]
traf_reg[,year:=factor(year)]

# event study regression using all the event time indicators
es1 <- lm(ln_fat_pc ~ .-`event_time_-1000`, data = traf_reg)
es1_cluster <- vcovCL(es1, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es1, vcov. = es1_cluster)

# lm chooses to remove event time 19

```

When we estimate an event study regression using all the event time indicators, the $\texttt{lm}$ function in R chooses to remove event time 19. It makes sense that R omitted one event time indicator because in practice, one cannot estimate the regression using all the event time indicators because of the dummy variable trap. An event study regression using all the event time indicators contains $T$ dummy variables, which fully saturates event time and leads to the dummy variable trap.

(b) Estimate another event study regression using all the event time indicators save one that you choose to omit. Generate a plot of the event study coefficients. 

```{r, include = TRUE}

# omit event_time -1
es2 <- lm(ln_fat_pc ~ .-`event_time_-1`-`event_time_-1000`, data = traf_reg)
es2_cluster <- vcovCL(es2, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es2, vcov. = es2_cluster)

es2_table <- data.table(tidy(coeftest(es2, vcov. = es2_cluster)))

es2_table <- es2_table[grep("event_time", term),]
es2_table[, event_time := as.numeric(str_match(es2_table$term, "([0-9]{1,2})")[, 2])]
es2_table[grep("-", term), event_time := -1 * event_time]
es2_table[, ci_upper := estimate + 1.96*std.error]
es2_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es2_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()

```

(c) Create minimum and maximum event time indicators that correspond to bins of event time $< -5$ and event time $> 5$ respectively. Appropriately specify and estimate an event study regression using these min and max event time indicators. Generate a plot of the event study coefficients. Explain which specification you prefer, this one or the one in part (b). 

```{r, include = TRUE}

# create min and max event time indicators for event time < -5 and > 5
traf_pt1 <- traf_pt1[, bin_event_time := event_time]

# make any event time > 5 equal to 6 (max event time indicator)
setDT(traf_pt1)[, bin_event_time := ifelse(event_time > 5, 6, bin_event_time)]

# make any event time < -5 equal to -6 (min event time indicator)
# this means never treated states (which have event time -1000) will be in the
# min event time group, but this doesn't change the coefficient on the min 
# event time dummy 
setDT(traf_pt1)[, bin_event_time := ifelse(event_time < -5, -6, bin_event_time)]


# make separate bin event time indicator for each possible value of bin event time in the data set 
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("bin_event_time"))]

# interact with ever treated
bin_event_time_cols <- names(traf_pt1[,`bin_event_time_-1`:`bin_event_time_6`]) # get list of bin event time columns
# multiply ever treated by all bin event time columns
traf_pt1 <- traf_pt1[, (bin_event_time_cols) := lapply(.SD, function(x) x * traf_pt1$treat), 
                     .SDcols = bin_event_time_cols]

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg3 <- traf_pt1[,c("ln_fat_pc", "state", "year",..bin_event_time_cols)]
traf_reg3[,state:=factor(state)]
traf_reg3[,year:=factor(year)]

# event study regression using all the bin event time indicators except -1
es3 <- lm(ln_fat_pc ~ .-`bin_event_time_-1`, data = traf_reg3)
es3_cluster <- vcovCL(es3, type = "HC1", cluster=traf_reg3$county) # get cluster cov-var
coeftest(es3, vcov. = es3_cluster)

es3_table <- data.table(tidy(coeftest(es3, vcov. = es3_cluster)))

es3_table <- es3_table[grep("event_time", term),]
es3_table[, event_time := as.numeric(str_match(es3_table$term, "([0-9]{1,2})")[, 2])]
es3_table[grep("-", term), event_time := -1 * event_time]
es3_table[, ci_upper := estimate + 1.96*std.error]
es3_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es3_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()

```

There are tradeoffs between this specification (with min and max bins) and the specification in part (b) (without min and max bins). The specification in part (b) allows us to look at estimates for more years before and after treatment. However, the very large and very small values of event time may include information from only a few states since the distribution of event time is unbalanced across units in our sample. Early-treated units do not have very early event time dummies, and late-treated units do not have very late event time dummies. 

The min and max bins in this specification (c) partially address the unbalanced distribution of event time across units, but this specification only shows us 6 periods before and 6 periods after treatment. Additionally, the cutoffs for min and max in this specification can seem arbitrary. Nevertheless, as we will see in part (d), the min and max bins in this specification help avoid collinearity that can occur with the specification in part (b) if there are no "pure control" states. So overall, this specification with min and max bins seems preferable.

(d) What happens to your estimates from part (b) if you exclude the "pure control" states from your sample? What about if you exclude the pure controls in part (c)?

```{r, include = TRUE}

# exclude "pure control" states from the sample
traf_pt1_no_pure <- traf_pt1[!(event_time==-1000),]

# redo part (b) with no "pure control" states

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg <- traf_pt1_no_pure[,c("ln_fat_pc", "state", "year",..event_time_cols)]
traf_reg[,state:=factor(state)]
traf_reg[,year:=factor(year)]

es_b <- lm(ln_fat_pc ~ .-`event_time_-1`, data = traf_reg)
es_b_cluster <- vcovCL(es_b, type = "HC1", cluster=traf_reg$county) # get cluster cov-var
coeftest(es_b, vcov. = es_b_cluster)

# lm chooses to remove event time 19
# (even though we already removed event time -1)
```

If we exclude the "pure control" states from our sample, the $\texttt{lm}$ function in R chooses to remove event time 19, even though we already removed one event time dummy (event time -1). It makes sense that R omitted an additional event time dummy because without any "pure control" states, there is collinearity since event time is equal to calendar time minus the treatment date. When there are no "pure control" states, the full set of event time indicators (with one omitted) plus time and treatment date fixed effects (treatment date fixed effects are proxied by state fixed effects) will be collinear and thus result in the dummy variable trap, which is why it makes sense that R removed an additional event time dummy.


```{r, include = TRUE}
# redo part (c) with no "pure control" states

# make table with just the variables for the regression (makes writing out regression equation easier)
traf_reg3 <- traf_pt1_no_pure[,c("ln_fat_pc", "state", "year",..bin_event_time_cols)]
traf_reg3[,state:=factor(state)]
traf_reg3[,year:=factor(year)]

# event study regression using all the bin event time indicators except -1
es_c <- lm(ln_fat_pc ~ .-`bin_event_time_-1`, data = traf_reg3)
es_c_cluster <- vcovCL(es_c, type = "HC1", cluster=traf_reg3$county) # get cluster cov-var
coeftest(es_c, vcov. = es_c_cluster)

es_c_table <- data.table(tidy(coeftest(es_c, vcov. = es_c_cluster)))

es_c_table <- es_c_table[grep("event_time", term),]
es_c_table[, event_time := as.numeric(str_match(es_c_table$term, "([0-9]{1,2})")[, 2])]
es_c_table[grep("-", term), event_time := -1 * event_time]
es_c_table[, ci_upper := estimate + 1.96*std.error]
es_c_table[, ci_lower := estimate - 1.96*std.error]

ggplot(data=es_c_table, aes(x=event_time, y=estimate)) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.1)  +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Event Study for Primary Laws") +
  theme_bw()
```

If we exclude the "pure control" states from our sample, the estimates as compared to part (c) are slightly different, especially for the larger and smaller event time bins, and more coefficients were statistically significant in (c) than here. But overall, the estimates are not that different from (c). Importantly, there is no collinearity in this min and max bin specification, even when there are no "pure control" states, which shows that collinearity was avoided using the specification with min and max bins, but it was not avoided when redoing the specification in (b) with no "pure control" states. 

(e) Overall, does the event study regression make you more confident or less confident that seat belt laws reduce fatalities (relative to the fixed effects results that you estimated on the last problem set)? Briefly explain. 

Last problem set, we found that when we add our full set of covariates to the FE estimator, having primary belt laws is associated with 8.98% decrease in traffic fatalities per capita, ceteris paribus, which is statistically significant at the 1% level. Overall, the event study regression makes us less confident that seat belt laws reduce fatalities (relative to our fixed effects results that we estimated last problem set). From the lecture notes, for event studies, an abrupt change in the outcome immediately after treatment is often viewed as more credible than a gradual change over time, because it is less likely to be due to differential trends between the treated and control states. Our plots of the event study coefficients do not show an abrupt (or statistically significant) change in the outcome immediately after treatment, or even in the first few periods after treatment. So our event study regression results make us less confident. However, our event study regression specification may need to be modified further before interpreting the results, and we will do one such modification in part (f).

(f) Building off the event study regression from part (c), estimate the interaction weighted event study estimator from Sun and Abraham (2020) (discussed in lecture and posted on bCourses for refrence). As a reminder, the interacted event study regression takes the standard event time indicators (without any binning) and interacts each one with a cohort indicator (a cohort refers to a group of states that share the same date on which they were first treated). You then form the estimate for event time coefficient $\tau_j$ by averaging the estimates of the cohort-specific $\tau_j$ using the weights described in Sun and Abraham (2020).

```{r}
## f interaction weighted
# get cohorts
traf_pt1[event_time==0,cohort:=year]
traf_pt1[,cohort:= lapply(.SD, function(x) replace(x, is.na(x), mean(x, na.rm=T))), .SDcols = "cohort", by = state]
traf_pt1[is.na(cohort), cohort:=2103]
# get dummy for each cohort
traf_pt1 <- traf_pt1[,dummy_cols(traf_pt1, select_columns = c("cohort"))]
cohort_cols <- names(traf_pt1[,cohort_1984:cohort_2003]) # get list of bin event time columns

traf_reg_iw <- traf_pt1[,c("ln_fat_pc", "state", "event_time", "cohort", "year",..event_time_cols,..cohort_cols)]
traf_reg_iw[,state:=factor(state)]
traf_reg_iw[,year:=factor(year)]

# get interactions
reg_iw_interactions <- function(x) {
  
  dt <- traf_reg_iw[cohort==x,]
  min_et <- 1981 - x # min interaction
  max_et <- 2003 - x # max interaction
  
  # don't create interactions for never treated
  if (x != 2103) {
    # loop through all in between min and max
    for (i in min_et:max_et) {
      dt[, glue("int_{x}_{i}") := eval(as.name(glue("event_time_{i}"))) * eval(as.name(glue("cohort_{x}")))]
    }
  }
  
  else {
  }
  
  return(dt)
}

cohorts <- unique(traf_reg_iw$cohort) # unique cohorts
int_dt_list <- lapply(cohorts, reg_iw_interactions) # lapply through all cohorts
int_dt <- rbindlist(int_dt_list, fill = T) # get as table
int_cols <- names(int_dt[,grep("int", names(int_dt)), with = F])
int_dt[,(int_cols):= lapply(.SD, function(x) replace(x, is.na(x), 0)), .SDcols = int_cols]
traf_reg_iw_final <-  int_dt[,c("ln_fat_pc", "state", "year",..int_cols)]

# run regression. omitted cohort is never treated. omitted event time is -1
es_iw <- lm(ln_fat_pc ~. - `int_2000_-1` - `int_1993_-1` - `int_1986_-1` - `int_2003_-1` -
              `int_1996_-1` - `int_1998_-1` - `int_1987_-1` - `int_1984_-1` - `int_1991_-1` - `int_2002_-1`, 
            data = traf_reg_iw_final)

# get results
iw_tab <- data.table(tidy(coeftest(es_iw)))
iw_tab <- iw_tab[grep("int", term),]
iw_tab[, cohort := as.numeric(str_extract(iw_tab$term, "([0-9]{4})"))]
iw_tab[, event_time := -1*as.numeric(str_match(iw_tab$term, "_-([0-9]{1,2})")[,2])]
iw_tab[is.na(event_time), event_time := as.numeric(str_match(iw_tab[is.na(event_time),term], "([0-9]{1,2})$")[,2])]

# get number of states
state_dt <- traf_pt1[,.(cohort = mean(cohort)), by = state]
state_dt <- state_dt[,.(num_states = .N), by = cohort]

iw_tab_merge <- merge(iw_tab, state_dt, by = "cohort")
# weighted average across cohorts
iw_tab_weight1 <- iw_tab_merge[,.(estimate = weighted.mean(estimate, w = num_states),
                                  num_states = .N), by = event_time]
# weighted average for t < -5 and t > 5
iw_tab_weight1[,time_bin := event_time]
iw_tab_weight1[event_time < -5,time_bin := -6]
iw_tab_weight1[event_time > 5,time_bin := 6]

iw_tab_weight2 <- iw_tab_weight1[,.(estimate = weighted.mean(estimate, w = num_states)), by = time_bin]

# plot
ggplot(data=iw_tab_weight2, aes(x=time_bin, y=estimate)) +
  geom_point() +
  geom_point(aes(x=-1, y=0), colour="black") +
  geom_hline(yintercept=0, colour="black") +
  geom_vline(xintercept = -1, colour = "red", linetype = "dashed") +
  xlab("Event Time") +
  ylab("Estimate") +
  ggtitle("Interaction Weighted Event Study") +
  theme_bw()

```


# Question 2

We now apply the synthetic control methods from Abadie et al (2010).

(a)

i. Compare the average pre-period log traffic fatalities per capita of the TU site to that of the average of all the “control” states. Next, graph the pre-period log traffic fatalities by year for the pre-period for both the TU and the average of the control group. Interpret.

```{r}
# #Create a treatment status variable that = 1 if state is CT, IA, NM, or TX and =0 otherwise. 
# traffic[,treat := ifelse(state_name == "CT" | state_name == "IA"|state_name == "NM" | state_name == "TX" |state == 99, 1, 0)]
# 
# traffic_TU <- traffic[treat == 0 | state == 99,]
# #Change treatment variable to a factor variable
# traffic_TU$treat <- as.factor(traffic$treat)
# 
# #Create log fatalities per capita variable
# traffic_TU[, ln_fat_pc := log(fatalities/population)]
# 
# #Compare the average pre-period log traffic fatalities per capita between treatment and control
# premeanT <- mean(traffic_TU[treat == 1 & year<1986, ln_fat_pc]) #mean pre-period log traffic fatalities in treatment
# premeanC <- mean(traffic_TU[treat == 0 & year<1986, ln_fat_pc]) #mean pre-period log traffic fatalities in control
# 
# #Create variable of mean log traffic fatalities by treatment status by year
# traffic_TU[, mean_lnfat_treat := lapply(.SD, mean), .SDcols = c("ln_fat_pc"), by = c("treat","year")] 
# 
# #Graph the mean pre-period log traffic fatalities by year for Treatment vs Control
# traffic_TU[year < 1986,] %>%
#   ggplot(aes(x=year, y = mean_lnfat_treat, group = treat, color = treat)) + 
#   geom_line() +
#   theme_minimal() +
#   labs(title = "Average Pre-Period Log Traffic Fatalities by Year", x = "Year", y = "Log Traffic Fatalities Per Capita", color = element_blank()) +
#   theme(plot.title = element_text(hjust = 0.5)) +
#   scale_color_manual(labels = c("Controls", "TU"), values = c("coral1", "cyan3"))

```

The average pre-period log traffic fatalities per capita in our aggregate treatment unit is premean t compared with premean c  in our control states. Graphically, we can see that while log traffic fatalities per capita are declining over time in both groups, treatment units have on average higher traffic fatalities per capita than control units in all pre-period years. This makes sense since states with higher traffic fatalities may make reducing traffic fatalities a bigger policy priority, leading to a higher likelihood of implementing seat belt laws.  

```{r}
#generate a variable that is the absolute value of the difference between the dependent variable of the TU site and each control state for the year before treatment 
# TU_dep_1985 <- traffic_TU[state == 99 & year == 1985, ln_fat_pc]
# traffic_TU[year == 1985,compare_dep := abs(ln_fat_pc - TU_dep_1985)]
# View(traffic_TU[compare_dep == min(traffic_TU$compare_dep),.(state, state_name)])
# apply(traffic_TU[year == 1985,ln_fat_pc])

```




