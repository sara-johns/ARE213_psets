---
title: "ARE 213 Problem Set 1B"
author: "Becky Cardinali, Yuen Ho, Sara Johns, and Jacob Lefler"
date: "Due 10/12/2020"
header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath}
output: pdf_document
fig_caption: yes
geometry: margin=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#======================
# Section 0: Set Up
#======================

# Clear workspace
rm(list = ls())

# install.packages("tidyverse")
# install.packages("MatchIt")

# Load packages
library(pacman)
library(tidyverse)
library(MatchIt)
p_load(data.table, dplyr, foreign, readstata13, tidyr, xtable, ggplot2, binom, glmnet, stats)

# Directory 
# base_directory <- "/Users/sarajohns/Desktop/ARE213_psets/"
#base_directory <- "/Users/beckycardinali/Desktop/ARE213_psets/"
base_directory <- "/Users/yuen/Documents/GitHub/ARE213_psets/"
# base_directory <- "C:\\Users\\jacob\\Documents\\GitHub\\ARE213_psets\\"

# read data
mom_dt <- read.dta(paste0(base_directory, "ps1.dta"))

# clean data, as in pset1a

# According to the codebook, for the following medical risk factor variables, 8 corresponds to 
# "Factor not on certificate" and 9 corresponds to "Factor not classifiable": cardiac, lung, diabetes, 
#herpes, chyper, phyper, pre4000, preterm

med_risk_factors <- c('cardiac', 'lung', 'diabetes', 'herpes', 'chyper', 'phyper', 'pre4000', 'preterm')

# Below, arr.ind = TRUE returns the indices at which the row equals a certain value
for (var in med_risk_factors){
  mom_dt[var] <- replace(mom_dt[var], which(mom_dt[var] == 8, arr.ind = TRUE), NA)
  mom_dt[var] <- replace(mom_dt[var], which(mom_dt[var] == 9, arr.ind = TRUE), NA)
}

# According to the codebook, for tobacco, 9 corresponds to "Unknown or not stated"
mom_dt$tobacco <- replace(mom_dt$tobacco, which(mom_dt$tobacco == 9, arr.ind = TRUE), NA)

# According to the codebook, for cigar, 99 corresponds to "Unknown or not stated"
mom_dt$cigar <- replace(mom_dt$cigar, which(mom_dt$cigar == 99, arr.ind = TRUE), NA)

# According to the codebook, for cigar6, 6 corresponds to "Unknown or not stated"
mom_dt$cigar6 <- replace(mom_dt$cigar6, which(mom_dt$cigar6 == 6, arr.ind = TRUE), NA)

# According to the codebook, for alcohol, 9 corresponds to "Unknown or not stated"
mom_dt$alcohol <- replace(mom_dt$alcohol, which(mom_dt$alcohol == 9, arr.ind = TRUE), NA)

# According to the codebook, for drink, 99 corresponds to "Unknown or not stated"
mom_dt$drink <- replace(mom_dt$drink, which(mom_dt$drink == 99, arr.ind = TRUE), NA)

# According to the codebook, for drink5, 5 corresponds to "Unknown or not stated"
mom_dt$drink5 <- replace(mom_dt$drink5, which(mom_dt$drink5 == 5, arr.ind = TRUE), NA)

# According to the codebook, for wgain (assuming that's wtgain in codebook), 
# 99 corresponds to "Unknown or not stated"
mom_dt$wgain <- replace(mom_dt$wgain, which(mom_dt$wgain == 99, arr.ind = TRUE), NA)

mom_dt <- na.omit(mom_dt)

setDT(mom_dt)
# Recode tobacco
mom_dt[, tobacco := ifelse(tobacco==2, 0, 1)]

# We are left with 114,610 observations after cleaning, as expected from the problem set instructions
```

# Question 1

In Problem Set 1a, you used linear regression to relate infant health outcomes and maternal smoking during pregnancy. Please answer the following questions. 

(a) Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in Problem Set 1a? 

\bigskip

Even if our assumption of "selection on observables" holds, our estimates from Pset1a may be biased if $E[D|X]$ is not linear in X. That is our estimates from Pset1a impose a linear functional form on the relationship between $Y$ and $X$, or between $E[D|X]$ and $X$, causing our estimates to be biased if the true relationship is not linear. To address the potential misspecification bias, we can instead use a nonparametric method to control for X, or estimate $E[D|X]$ nonparametrically. 

\bigskip

(b) Consider a series estimator. Estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions). What are the benefits and drawbacks to this approach? 

\bigskip

Recall from Pset1a that our list of predtermined variables, which for convenience we'll call $X$, were state of residence, age of mother, hispanic origin of mother, race of mother, educ of mother, total birth order, interval since last live birth, age of father, hispanic origin of father, month of birth, plurality, previous infant 4000 or more grams, previous preterm infant, prior birth, marital status of mother, and educ of father.

\bigskip

In our series estimator, in addition to $X$ above, to allow for a more flexible functional form we also include higher order terms of mother's and father's age and education. We also include interaction terms between mother's race and mother's age and splines for mother's age with knots at 18 and 35 (because pregnant women under the age of 18 or above the age of 35 may have higher health risks with pregnancy) and splines for mother's education with a knot at 12, for highschool completion. Specifically, we estimate the model: 

\begin{equation*}
\begin{gathered}
birthweight_i = \alpha_i + \beta smoking_i + \sum_{j = 1}^J \delta_j X_{ji} +  \gamma_1 mother\_age_i^2 + \gamma_2 mother\_age_i^3 + \gamma_3 mother\_educ_i^2 + \gamma_4 mother\_educ_i^3 + \\
\gamma_{5} father\_age_i^2 + \gamma_{6} father\_age_i^3 + \gamma_{7} father\_educ_i^2 + \gamma_{8} father\_educ_i^3 + \gamma_{9} mother\_race_iXmother\_age_i + \\
\gamma_{10} mother\_race_iXmother\_age_i^2 + \gamma_{11} I (mother\_age_i > 18) (mother\_age_i - 18)^3 + \\
\gamma_{12} I (mother\_age_i > 35) (mother\_age_i - 35)^3 + 
\gamma_{13} I (mother\_educ_i >= 12) (mother\_educ_i - 12)^3 + \epsilon_i
\end{gathered}
\end{equation*}

Where $X_j = (X_1, ... X_J)$ represents the J pre-determined variables listed above. 

```{r, include = TRUE}
# Generate higher order terms
mom_dt[,c("dmage2", "dmage3", "dfage2", "dfage3", "dmeduc2", "dmeduc3", "dfeduc2", "dfeduc3") := .(dmage^2, dmage^3, dfage^2, dfage^3, dmeduc^2, dmeduc^3, dfeduc^2, dfeduc^3)]

# Generate splines for mother's age above 18 and above 35
mom_dt[dmage >= 18 , dmage_adult := (dmage - 18)^3]
mom_dt[dmage < 18, dmage_adult := 0]
mom_dt[dmage > 35, dmage_ger := (dmage-35)^3]
mom_dt[dmage <= 35, dmage_ger := 0]
#View(mom_dt[,.(dmage, dmage_adult, dmage_ger)])

# Generate splines for mother's education highschool graduate or more
mom_dt[dmeduc >= 12, dmeduc_hs := (dmeduc-12)^3]
mom_dt[dmeduc < 12, dmeduc_hs := 0]
#View(mom_dt[,.(dmeduc, dmeduc_dropout, dmeduc_hs)])

lm1 <- lm(dbrwt ~ tobacco + factor(stresfip) + factor(mrace3) + factor(birmon) + factor(orfath) + factor(ormoth) + factor(pre4000) + factor(preterm) + factor(dmar) + dtotord + disllb + dplural + dmage + dmage2 + dmage3 + dmeduc + dmeduc2 + dmeduc3 + dfage + dfage2 + dfage3 + dfeduc + dfeduc2 + dfeduc3 + factor(mrace3)*dmage + factor(mrace3)*dmage2 + dmage_adult + dmage_ger + dmeduc_hs, mom_dt)

summary(lm1)
```

A benefit to this approach is that it is straightforward and relatively easy to implement and interpret. However, a drawback to this approach is that the choice of which covariates, higher order terms, interaction terms, and knots to include is arbitrary, so our series estimator may not accurately capture the true data generating process and we run the risk of overfitting our model or omitting an important control variable.

\bigskip

(c) Use the LASSO to determine which covariates (and higher order terms) to include in your regression from part (b). Do you end up dropping some covariates that you had thought might be necessary to include? 

\bigskip

We follow the procedure suggested by Belloni, Chernozhukov, and Hansen by first applying the LASSO to the equation in part (b). We also apply the LASSO to the model regressing treatment status on the remaining covariates: 

\begin{equation*}
\begin{gathered}
smoking_i = \pi_0 + \sum_{j = 1}^J \pi_j X_{ji} + \nu_1 mother\_age_i^2 + \nu_2 mother\_age_i^3 + \nu_3 mother\_educ_i^2 + \nu_4 mother\_educ_i^3 + \\
\nu_{5} father\_age_i^2 + \nu_{6} father\_age_i^3 + \nu_{7} father\_educ_i^2 + \nu_{8} father\_educ_i^3 + \nu_{9} mother\_race_iXmother\_age_i + \\
\nu_{10} mother\_race_iXmother\_age_i^2 + \nu_{11} I (mother\_age_i > 18) (mother\_age_i - 18)^3 + \\
\nu_{12} I (mother\_age_i > 35) (mother\_age_i - 35)^3 + 
\nu_{13} I (mother\_educ_i >= 12) (mother\_educ_i - 12)^3 + \epsilon_i
\end{gathered}
\end{equation*}
Then we regress $birthweight_i$ on $smoking_i$ and all of the covariates selected by LASSO in either equation 1 or 2 above. In each case, we cross-validate the LASSO and choose lambda such that the mean cross-validated error is within 1 standard error of the minimum. 

```{r, include = TRUE}
#First convert factor variables in dataset to factor variables
mom_dt[,stresfip := as.factor(stresfip)]
mom_dt[,cntocpop := as.factor(cntocpop)]
mom_dt[,mrace3 := as.factor(mrace3)]
mom_dt[,dmar := as.factor(dmar)]
mom_dt[,birmon := as.factor(birmon)]
mom_dt[,orfath := as.factor(orfath)]
mom_dt[,ormoth := as.factor(ormoth)]
mom_dt[,pre4000 := as.factor(pre4000)]
mom_dt[,preterm := as.factor(preterm)]

#Lasso for lm1
x1 <- model.matrix(lm1, data = mom_dt) #create X matrix
lasso_y <- cv.glmnet(x1, mom_dt$dbrwt, family = "gaussian", standardize = TRUE, intercept = TRUE, alpha = 1, type = "mse")
#plot(lasso_y)
#lasso_y$glmnet.fit
coef(lasso_y, s = "lambda.1se") #Coefficients from lasso for lambda that gets error within 1 se of the minimum

#Lasso for model of treatment status as a function of the other control variables
lm2 <- lm(tobacco ~ factor(stresfip) + factor(mrace3) + factor(birmon) + factor(orfath) + factor(ormoth) + factor(pre4000) + factor(preterm) + factor(dmar) + dtotord + disllb + dplural + dmage + dmage2 + dmage3 + dmeduc + dmeduc2 + dmeduc3 + dfage + dfage2 + dfage3 + dfeduc + dfeduc2 + dfeduc3 + factor(mrace3)*dmage + factor(mrace3)*dmage2 + dmage_adult + dmage_ger + dmeduc_hs, mom_dt) 

x2 <- model.matrix(lm2, data = mom_dt) #create X matrix
lasso_d <- cv.glmnet(x2, mom_dt$dbrwt, family = "gaussian", standardize = TRUE, intercept = TRUE, alpha = 1, type = "mse")
#plot(lasso_d)
#lasso_d$glmnet.fit
coef(lasso_d, s = "lambda.1se") #Coefficients from lasso for lambda that gets error within 1 se of the minimum

```

Based on these results, we choose to keep the following covariates: an intercept, tobacco, both levels of mother's race, a dummy for whether the mother is Puerto Rican, a dummy for whether the mother is Hispanic "other", previous infant 4000 grams or more, previous preterm infant, marital status, interval since last live birth, plurality, mother's education and the square of mother's education, father's education and the square of father's education, a spline for mother's age with a knot at 35, and the interaction term between whether the mother is black by mother's age. 

\bigskip

We drop a significant number of covariates that we had initially thought would be necessary to include, such as state of residence, birth month, hispanic origin of father, the cube of mother's and father's education, father's age and its higher order terms, mother's age and its higher order terms, and most of the spline and interaction terms. 

\bigskip

After our LASSO procedure, our final model is: 

\begin{equation*}
\begin{gathered}
birthweight_i = \alpha_i + \beta Smoking_i + \delta_1 mother\_race_i + \delta_2 mother\_hispanic_i == 2 + \delta_3 mother\_hispanic_i == 5 + \delta_4 pre4000_i + \\
\delta_5 preterm_i + \delta_6 marital\_status_i + \delta_7 last\_birth_i + \delta_8 plurality_i + \delta_9 mother\_educ_i + \delta_{10}mother\_educ_i^2 + 
\delta_{11}father\_educ_i + \\
\delta_{12}father\_educ_i^2 + \delta_{13} I (mother\_age_i > 35) (mother\_age_i - 35)^3 + \delta_{14} mother\_race_i == 3 X mother\_age_i + \epsilon_i
\end{gathered}
\end{equation*}

```{r, include = TRUE}
#Create necessary dummy and interaction terms
mom_dt[, ormoth2 := ifelse(ormoth == 2, 1, 0)]
mom_dt[,ormoth2 := as.factor(ormoth2)]
mom_dt[,ormoth5 := ifelse(ormoth == 5, 1, 0)]
mom_dt[,ormoth5 := as.factor(ormoth5)]
mom_dt[,mrace3Xdmage := ifelse(mrace3 == 3, 3*dmage, 0)]

#New model with covariates chosen from 2-stage Lasso above
lm3 <- lm(dbrwt ~ tobacco + factor(mrace3) + factor(ormoth2) + factor(ormoth5) + factor(pre4000) + factor(preterm) + factor(dmar) + disllb + dplural + dmeduc + dmeduc2 + dfeduc + dfeduc2 + dmage_ger + mrace3Xdmage, mom_dt)
summary(lm3)
```
# Question 2

Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality problem of multivariate matching? 

\medskip

We know that if we condition on observables, we will get a consistent estimate of the ATE under the selection on observables assumption. However, if the observables are high dimensional, it might be difficult to find a comparison unit with the same values of the observables. From lecture, we know that it is sufficient instead to condition on the propensity score. Using the propensity score allows us to compare treated and control units with the same probability of being treated, controlling for any differences that are consistently related to the probability of treatment. The propensity score does not require that all values of the observables be the same and so avoids problems of multidimensionality.

\bigskip

Try a few ways to estimate the effects of maternal smoking on birthweight:

(a) First create the propensity score. For our purposes let's use a logit specification. First specify the logit using all of the "predetermined" covariates (don't include interactions). Next, include only those "predetermined" covariates that enter significantly in the first logit specification. How comparable are the propensity scores? If they are similar does this imply that we have the "correct" set of covariates in the logit specification used for our propensity score?

```{r}

# get prop score using all predetermined variables
prop_all <- glm(tobacco ~ factor(stresfip) + dmage + factor(mrace3) + dmeduc + 
                  dtotord + disllb + dfage + factor(birmon) + factor(orfath) + factor(ormoth) +
                  factor(dmar) + dfeduc + dplural + factor(pre4000) + factor(preterm),
                family=binomial(link='logit'), data = mom_dt)
mom_dt[, prop_score_all := fitted(prop_all)]

# take a look at the output to see which are significant - omitting because takes up a lot of space
# summary(prop_all)
# only need to take out state of residence. birth month has a few months that are significant so will keep

# get prop score using significant variables from previous logit
prop_sig <- glm(tobacco ~ dmage + factor(mrace3) + dmeduc + 
                  dtotord + disllb + dfage + factor(birmon) + factor(orfath) + factor(ormoth) +
                  factor(dmar) + dfeduc + dplural + factor(pre4000) + factor(preterm),
                family=binomial(link='logit'), data = mom_dt)
mom_dt[, prop_score_sig := fitted(prop_sig)]

# how different are the prop scores?
prop_score_diff <- mom_dt$prop_score_all - mom_dt$prop_score_sig
summary(prop_score_diff)
# a few outliers but not very different

```

In the first logit, we include state of residence, mother's age, mother's race, mother's hispanic origin, mother's education, birth order, interval since last birth, father's age, father's Hispanic origin, father's education, birth month, plurality, previous heavy child, and previous preterm. This is a combination of the "predetermined" variables we selected and discussed in the last problem set and the ones discussed in the problem set solutions. We find that all coefficients are significant, except for state of residence. We remove that from the next regression and the propensity scores, which are the fitted values of the logit, do not change very much. This does not guarantee we are estimating the logit with the correct set of covariates. There could be important covariates that we do not have in this dataset that we would want to use in the logit specification. The fact that our propensity scores do not change much between the two specifications just gives us confidence that state of residence was not an important regressor to include. Moving forward, we will use the propensity scores from the second specification.

\bigskip

(b) Control directly for the estimated propensity scores using a regression analysis, and estimate an average treatment effect. State clearly the assumptions under which your estimate is correct.

```{r}

# run regression with prop_score
prop_reg <- lm(dbrwt ~ tobacco + prop_score_sig, data = mom_dt)

```

Controlling directly for the propensity score, we get the ATE is `r round(prop_reg$coefficients[[2]], 2)` and is statistically significant at the 99.9\% level. The assumption under which this estimate is consistent is unconfoundedness and homogeneous treatment effects. If we instead believed that there were heterogeneous treatment effects (that varied with $X$), then this regression does not provide meaningful results.

\bigskip

(c) As discussed in class, one can use the estimated propensity scores to reweight the outcomes of non-smokers and estimate the average treatment effect. Compute an estimate of the average treatment effect and the "effect of the treatment on the treated" by appropriate reweighting of the data. 

```{r}

# create propensity weights
mom_dt[,prop_weights := ifelse(tobacco == 1, 
                          1/prop_score_sig, 
                          1/(1 - prop_score_sig))] 
# normalize the weights
mom_dt[,norm_prop_weights := ifelse(tobacco == 1, 
                               prop_weights/sum(mom_dt[tobacco == 1, prop_weights]),
                               prop_weights/sum(mom_dt[tobacco == 0, prop_weights]))] 
# estimate ATE
tau_ipw <- sum((mom_dt$tobacco*mom_dt$dbrwt)*(mom_dt$norm_prop_weights) - 
                 ((1 - mom_dt$tobacco)*mom_dt$dbrwt)*(mom_dt$norm_prop_weights))

# estimate TOT - use formula from section 
tot_y1 <- sum(mom_dt$tobacco*mom_dt$dbrwt) / sum(mom_dt$tobacco) 
tot_y0 <- sum(mom_dt$prop_score_sig * (1 - mom_dt$tobacco) * mom_dt$dbrwt / 
        (1 - mom_dt$prop_score_sig)) /
  sum(mom_dt$prop_score_sig * (1 - mom_dt$tobacco) / 
        (1 - mom_dt$prop_score_sig))

tau_tot <- tot_y1 - tot_y0
```

We have from lecture that the ATE using inverse propensity weighting is 

$$ \hat{\tau}_{ATE} = \bigg(\sum^N \frac{D_iY_i}{\hat{p}(X_i)} / \sum^N \frac{D_i}{\hat{p}(X_i)}) - (\sum^N \frac{(1-D_i)Y_i}{1-\hat{p}(X_i)} / \sum^N \frac{1-D_i}{1-\hat{p}(X_i)}\bigg) $$ 

and using this formula, we get that the ATE is `r round(tau_ipw, 2)`. We have from section that the TOT using inverse propensity weighting is 

$$ \hat{\tau}_{TOT} = \bigg(\sum^N D_iY_i / \sum^N D_i - (\sum^N \frac{\hat{p}(X_i)(1-D_i)Y_i}{1-\hat{p}(X_i)} / \sum^N \frac{\hat{p}(X_i)(1-D_i)}{1-\hat{p}(X_i)}\bigg)  $$

and using this formula, we get that the TOT is `r round(tau_tot, 2)`.

\bigskip

(d) Estimate the counterfactual densities relevant for the above part with a kernel density estimator. That is, estimate the density of birthweight (or log birthweight) if everyone smoked and again if no one smoked. Hint: Consider directly applying the Hirano, Imbens, and Ridder propensity score reweighting scheme in the context of estimating the densities of the treated and control groups (rather than the means of the treated and control groups). Stata has very useful preprogrammed commands. In addition to using the preprogrammed Stata command to compute/graph the kernel density over the entire range of birthweight, please also calculate by hand the kernel estimator at birthweight equals 3,000 grams (and provide the code you wrote that shows the calculation of the kernel estimator at this single point). Play around with a bandwidth starting with half the default Stata bandwidth. Choose the same bandwidth for all the pictures, and produce a (beautiful, production quality) figure depicting both densities.

```{r}

d0 <- density(mom_dt[tobacco==0,dbrwt], kernel="gaussian", bw="sj", 
              adjust=1, weights=mom_dt[tobacco==0,norm_prop_weights])
d1 <- density(mom_dt[tobacco==1,dbrwt], kernel="gaussian", bw="sj",
              adjust=1, weights=mom_dt[tobacco==1,norm_prop_weights])

plot(d0, col="blue", main="Counterfactual Densities", 
     xlab = "Birthweight (g)")
lines(d1, col="red")
legend(1, 0.0008, legend=c("Non-smoker", "Smoker"), 
       col=c("blue","red"), lty=1, cex=0.8)

```

In order to get the counterfactual densities, we weight the treated and control groups by the normalized inverse propensity scores, similar to our calculation above. We use the density() function with a Gaussian kernel and a bandwidth using the methods of Sheather & Jones (1991) which uses pilot estimation of derivatives. This results in a bandwidth of `r round(d0$bw, 2)` for the control and a bandwidth of `r round(d1$bw, 2)` for the treated. As we expect from our estimates of the ATE, the density of the birthweights for the treated group is shifted to the left of the control group (lower birthweights).

```{r}

d_all <- density(mom_dt$dbrwt, kernel="gaussian", bw="sj", adjust=1)
plot(d_all, col="black", main="Kernel Density, Full range of birthweights", 
     xlab = "Birthweight (g)")

# kernel estimator at 3000 g
# use bandwidth selected above: h = 49
h <- d_all$bw
ke_3000 <- 1 / (nrow(mom_dt) * h * sqrt(2*pi)) * sum(exp(-0.5 * (((3000-mom_dt$dbrwt)/h)^2 )))

```

We plot the kernel density over the entire range of birthweights, using the Gaussian kernel and selecting the bandwidth using the methods of Sheather & Jones (1991) which uses pilot estimation of derivatives. For the entire range of birthweights, this gives us a bandwidth of `r round(d_all$bw, 2)`. We then calculate the kernel estimator by hand at a weight of 3000 grams using the Gaussian kernel and this bandwidth. Our formula is 

$$ \hat{f}(x) = \frac{1}{nh} \frac{1}{\sqrt{2 \pi}} \sum\limits_i^n e^{-1/2 (\frac{x - x_i}{h})^2} $$

where $x = 3,000$ , $n=$ `r nrow(mom_dt)`, and $h=$ `r round(d_all$bw, 2)`. We get that the density at 3000g is `r round(ke_3000, 5)`, which visually corresponds to our plot of the density.

\bigskip

(e) Take one of your densities and display an estimate of the density using different bandwidths as well as the one you settled on. What happens with bigger (smaller) bandwidths?

```{r}
# plot bandwidth selected
plot(d1, col="black", main="Kernel Density, Smoker: Selected Bandwidth", 
     xlab = "Birthweight (g)")
# bw of treated group is 68.9, what if we used 49.8 like control?
d1_low <- density(mom_dt[tobacco==1,dbrwt], kernel="gaussian", bw=49.8, adjust=1, 
                  weights=mom_dt[tobacco==1,norm_prop_weights])
plot(d1_low, col="black", main="Kernel Density, Smoker: Lower Bandwidth", 
     xlab = "Birthweight (g)")
# what if we raised it?
d1_high <- density(mom_dt[tobacco==1,dbrwt], kernel="gaussian", bw=200, adjust=1, 
                   weights=mom_dt[tobacco==1,norm_prop_weights])
plot(d1_high, col="black", main="Kernel Density, Smoker: Higher Bandwidth", 
     xlab = "Birthweight (g)")
# what if we made it very low
d1_verylow <- density(mom_dt[tobacco==1,dbrwt], kernel="gaussian", bw=20, adjust=1, 
                      weights=mom_dt[tobacco==1,norm_prop_weights])
plot(d1_verylow, col="black", main="Kernel Density, Smoker: Very Low Bandwidth", 
     xlab = "Birthweight (g)")

```

We use the treated group counterfactual density and try a few different bandwidths. First, we plot our selected bandwidth, `r round(d1$bw, 2)`. Next, since the treated group is smaller than the control group, the bandwidth selected had been larger than that of the control group, so we plot the density had we used the same bandwidth as the control, ~ 49.8. Next we crank up the bandwidth to 200 and finally we drop it to 20. The higher the bandwidth, the smoother the density. At a bandwidth of 20, the density becomes a lot choppier. This is what we expect as a lower bandwidth in a Gaussian kernel gives less weight to observations further from x, making $\hat{f}(x)$ more sensitive to observations closer to x, producing less smoothing.

\bigskip

(f) What are the benefits of the weighting approach (from part c)? What are the potential drawbacks? Pay particular attention to to the issue of people with extremely high and extremely low values of the propensity score.

\medskip

The benefits of the weighting approach in part c are that we can get consistent estimates of the ATE and TOT that are also efficient. One potential drawback is that we have to first estimate the propensity score, which we are not certain we estimate correctly. Additionally, we run into problems if there is insufficient overlap in the treatment and control distributions of the covariates. This in turn would cause insufficient overlap in the propensity score itself, where there would be ranges of $p(X)$ that contain many estimated scores from the treatment group but not from the control group, or vice versa. If the propensity score $p(X_i)$ gets close to zero or one, then the weights become enormous, making our estimation in part (c) above very sensitive to outliers. Specifically, since people with extremely high propensity scores would mostly be found in the treated group and people with extremely low propensity scores would mostly be found in the control group, any observations with extremely high propensity scores found in the control group, or with extremely low propensity scores found in the treatment group, would receive an enormous weight since our weighting scheme above balances the propensity score across treated and control group. Our estimation would then be very sensitive to a few observations, or outliers. To address this concern, we can trim observations above or below certain propensity scores to improve overlap between treatment and control, as in Imbens (2007). 

\bigskip

(g) Present your findings and interpret the results on the relationship between birthweight and smoking. For the estimates in parts (b) and (c), consider which of the following conditions must hold in order for that estimate to be valid:
A1. The treatment effect heterogeneity is linear in the propensity score.
A2. The treatment effect heterogeneity is not linear in the propensity score.
A3. The decision to smoke is completely randomly assigned.
A4. Conditional on the exogenous variables the decision to smoke is randomly assigned.

\medskip

Our findings are included with the relevant sections above. All of our results are consistent with one another in finding that smoking causes lower birthweights -- as indicated by our estimates of the ATE, the TOT, and the counterfactual densities of birthweight. In part (b), we include the propensity score in a regression. In order for this estimate to be valid, we need assumptions A1 and A4 to hold (assuming we also include an interaction term between treatment status and the propensity score to allow for heterogeneous treatment effects). In part (c), we again need assumption A4 to hold. 

# Question 3
A potentially more informative way to describe how birth weight affects smoking is to estimate the "non-parametric" conditional mean of birth weight as a function of the estimated probability of smoking, separately for smokers and non-smokers on the same graph. To do so, divide the data from smokers into 100 approximately equally spaced bins based on the estimated propensity score. Do teh same for nonsmokers. Use the blocking estimator we discussed in class. Interpret your findings and relate them to the results in (2b).

# Question 4
Low birth weight births (less than 2,500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo question 3 using an indicator for low birth weight births as the outcome of interest. Interpret your findings.

# Question 5

Let's link matching back to regression. Consider the conditional expectation function $E[ \ birthweight \ | \ X]$, where $X$ contains the following variables: \texttt{rectype pldel3 cntocpop stresfip dmage mrace3 dmar adequacy csex dplural}. 

## 5a

Develop a regression that you are confident estimates $E[ \ birthweight \ | \ X]$ as $N \rightarrow \infty$? Why are you confident that your regression gets the CEF right?

A regression of birthweight on a saturated model for the discrete regressors (i.e. for every combination of the variables in $X$ that appears in the data, there is a unique dummy variable) gets the CEF right as $N \rightarrow \infty$, assuming that as $N \rightarrow \infty$ we would add in more dummy variables for any additional new combinations of the variables in $X$ that appear in the data.

We are confident that this regression gets the CEF right because having a saturated model for discrete regressors is a sufficient condition for a linear CEF. Then we can use the Regression-CEF Theorem. More specifically:

From the codebook and data cleaning, we know 
\begin{itemize}
\item \texttt{rectype} is record type (resident or nonresident): a discrete variable with values $\in \{1, 2\}$
\item \texttt{pldel3} is place or facility of birth: a discrete variable with values $\in \{1, 2\}$
\item \texttt{cntocpop} is population size of county of occurrence: a discrete variable with values $\in \{0, 1, 2, 3\}$
\item \texttt{stresfip} is state of residence: a discrete variable with values $\in \{0, ..., 55\}$
\item \texttt{dmage} is age of mother: a discrete variable with values $\in \{12, 13, ..., 49\}$
\item \texttt{mrace3} is race of mother: a discrete variable with values $\in \{1, 2, 3\}$
\item \texttt{dmar} is marital status of mother: a discrete variable with values $\in \{1, 2\}$
\item \texttt{adequacy} is adequacy of care index: a discrete variable with values $\in \{1, 2, 3\}$ 
\item \texttt{csex} is child sex: a discrete variable with values $\in \{1, 2\}$
\item \texttt{dplural} is plurality (single, twin, triplet, etc): a discrete variable with values $\in \{1, 2, 3, 4\}$
\end{itemize}

Since all of the regressors are discrete, we can use a saturated model for discrete regressors. (From the lecture notes: A saturated model is one in which you estimate a separate parameter for each point in the support of $x_i$ (e.g., you have a separate dummy variable for each unique value of the vector $x_i$ in your data set)). This is a sufficient condition for the CEF to be linear. 

By the Regression-CEF Theorem, if the CEF is linear, then the regression of $y_i$ on $x_i$ estimates the CEF. Formally, if $E[y_i|x_i] = x_i\gamma$, then $\gamma = E[x_i'x_i]^{-1}E[x_i'y_i]$ (which is what the regression coefficient converges to).

\bigskip

## 5b

*Now run the regression you propose above, but add the treatment (your binary smoking variable) as the righthand side variable of interest. Prove that if the treatment effect of smoking on birthweight is independent of the covariates in $X$, then exact matching and your regression estimate the same thing. You may assume the conditional indpendence assumption holds given the variables in $X$ listed above. 

\bigskip

```{r}
# the following code came from Arthur's tip on the Slack channel
# when I try to run it though, it keeps running continuously

saturated_data <- mom_dt %>%
  select(dbrwt, tobacco, rectype, pldel3, cntocpop, stresfip, dmage, mrace3, 
         dmar, adequacy, csex, dplural) %>%
  group_by(rectype, pldel3, cntocpop, stresfip, dmage, mrace3, dmar, adequacy, 
           csex, dplural) %>%
  mutate(group = cur_group_id()) %>%
  ungroup()

#the line below is the one that takes forever to run for me
#reg <- lm(dbrwt ~ tobacco + factor(group), data=saturated_data)

```

Add proof

## 5c 

*Develop a weighted version of the exact matching estimator that estimates the same thing as the regression above (regardless of whether the treatment effect is independent of covariates). 

\bigskip
```{r}
# useful documentation on page 9 https://r.iq.harvard.edu/docs/matchit/2.4-20/matchit.pdf
# right now this is giving me no matched units
matched <- matchit(tobacco ~ rectype + pldel3 + cntocpop + stresfip + dmage + mrace3 + dmar + adequacy + csex + dplural, data=mom_dt, method="exact")
```

## 5d

*Estimate the weighted matching estimator you propose. Compare it to the regression estimate from part (b). Are they similar?

## 5e

*Is the sample size of your regression the same as the sample size of your matching estimator, or does the regression have more observations? If the regression has more observations, why don't these extra observations influence the treatment effect estimate? 

\bigskip
The sample size of the regression and the sample size of the matching estimator are not the same. The regression has more observations. This is because for the matching estimator, if a given cell doesn't have both a control and treatment observation, then that cell is dropped because we cannot estimate the treatment effect within that cell. But the regression doesn't drop any observations, it just gives 0 weight to those observations which, in our matching estimator, are in a cell that doesn't have both treatment and control.


## 5f

*Compute a standard error for your matching estimator using the formula from Imbens (2015). Specifically, note that your matching estimator should have a form $\frac{1}{N_t}\sum_{d_i=1} w_i y_i - \frac{1}{N_c} \sum_{d_i=0} w_i y_i$, where $\sum_{d_i=1} w_i = N_t$ and $\sum_{d_i=0} w_i = N_c$. Then the conditional variance is approximately $\sum_i (\frac{d_i}{N_t^2} + \frac{1-d_i}{N_c^2})w_i^2 \hat{\sigma}^2_{d_i}(x_i)$, where $\hat{\sigma}^2_{d_i}(x_i) = \frac{1}{2}(y_i - y_{nn(i)})^2$, and $y_{nn(i)}$ is the nearest neighbor to observation $i$ with the \textit{same} treatment status. Figure out the implicit weights $w_i$ in your estimator from part (d), and compute the conditional variance. Is it close to your regression coefficient variance? 

\bigskip

# Question 6

Concisely and coherently summarize your overall results, providing some intuition. Write it like you would the conclusion of a paper. In this summary, describe whether you think your best estimate of the effects of smoking is credibly identified. State why or why not. 

\bigskip